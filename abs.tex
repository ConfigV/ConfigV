
\begin{abstract}

Software failures resulting from configuration errors 
have been commonplaces as modern software systems 
become increasingly larger and more complex.
The lack of language constructs in configuration files, 
such as types and grammars, 
directed the focus of a configuration file verification 
towards building post-failure error diagnosis tools. 
In addition, the existing tools are generally language
specific, requiring the user to define at least a grammar for the language
models and explicit rules to check. 
In this paper, we propose a framework which analyzes
datasets of correct configuration files and derives rules for building a
language model for the given dataset. 
The resulting language model can be used
to verify new configuration files and detect errors in them. 
Our proposed framework is highly modular, 
does not rely on the system source code, and
can be applied to any new configuration file type with minimal user input.  
Our tool, named \app, relies on an abstract representation of language 
rules to allow for this modularity. 
\app supports learning of various rules, such as orderings,
value relations, and type errors, 
by defining a minimal interface for a rule, and by using a probabilistic types inference stategy.

\end{abstract}
