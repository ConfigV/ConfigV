\section{Learner}
\label{sec-learn}

The goal of the learner is to derive rules from the intermediate representation of the training set generated by the translator.
We describe an interface to define the different classes of rules that should be learned.
Each instance of the interface corresponds to a different class of configuration errors, as described in Sec.~\ref{sec:motiv}.

To learn rules over sets of configuration files, we use a generalization of \textit{association rule learning}~\cite{agrawal1993mining}, a technique very briefly summarized as inductive machine learning.
Association rule learning is used to learn how frequently items of a set appear together.
For example, by examining a list of food store receipts, we might learn that when a customer buys bread and peanut butter, the set of purchased items is also likely to include jelly.
Since configuration files have multiple complex relations, we extend these association relationships to generalized predicates.

In traditional association rule learning, rules take the form of an implication:
\begin{align*}
r = \{S_0,...,S_{|S|}\} \in\ valid\ \implies \{T_0,...,T_{|T|}\} \in\ valid
\end{align*}
where $S$ and $T$ are source and target sets of words.
This rule states that if the set of words $S$ appear in some $valid$ (i.e. non-rule breaking) file, the words $T$ must also appear in that file.
We generalize this relation so that a rule, $r$, instead takes the following form:
%
\begin{align*}
r = [S_0,...,S_{|S|}] \in \validSet \implies \validSet \vdash p([S_0,...,S_{|S|}],[T_0,...,T_{|T|}])
\end{align*}
%
This rule relates the keywords $S_i,T_j \subseteq \keys$ for $0 \leq i \leq |S|, 0 \leq j \leq |T|$ with a predicate $p$.
Whenever the set of keywords $S$ is present in some valid configuration file, \validSet, the predicate $p$ must hold over $S$ and $T$ given the context of \validSet.
In keeping with vocabulary from association rule learning, we refer to $S_r$ and $T_r$ as the source and target keyword lists of rule $r$ respectively.
A rule states that if the source list of keyword appears in a valid configuration file, the given predicate must hold between the source and target lists.
For clarity in notation, we define all predicates to have arity 2 by separating the source and target keywords into lists of length $|S|$ and $|T|$ respectively,
   but these can be equivalently thought of as predicates of arity $|S|+|T|$ as notated in Fig.~\ref{fig:ptypes}.

Taking the food store example, our learned rule would be:
\begin{align*}
[bread, butter] \in\ valid \implies\ valid \vdash assoc([bread,butter],[jelly])
\end{align*}
%
where the $assoc$ predicate denotes that the words must all appear in a file. 
In this way, our formalism is more expressive than the classic association rule learning problem.

The set $\keys$ is the set of unique keys from the training set (denoted \trainingSet) and the predicate $p$ is one of the classes of configuration errors.
The task of the learning algorithm is to transform a training set to a set of rules, weighted with \textit{support} and \textit{confidence}.
The set of rules learned from training set \trainingSet constitutes a specification for a configuration file to be considered correct.

The two metrics, support and confidence, are used in association rule learning, as well as other rule based machine learning techniques~\cite{han2007frequent,langley1995applications}.
We use slightly modified definitions of these, stated below, to handle arbitrary predicates as rules.
During the learning process, each rule is assigned a support and confidence to measure the amount and quality of evidence for the rule.
%
\begin{equation*}
 \textit{support(r)} = \frac{|\{C \in \trainingSet \mid S_r \cup T_r \subseteq C\}|} {|\trainingSet|}
\end{equation*}
\begin{equation*}
 \textit{confidence(r)} = \frac{|\{C \in \trainingSet \mid C\ \vdash p_r(S_r,T_r) \}| } {support(r)*|\trainingSet|}
\end{equation*}

Support is the frequency that the set of keywords in the proposed rule, $S \cup T$, have been seen in the configuration files $C$ in the training set \trainingSet.
%Notice support does not consider the predicate itself, only how many times the predicate had the chance to be evaluated.
Confidence is the percentage of times the rule predicate has held true over the given keywords.
In the learning process, each class of rule is manually assigned a support and confidence threshold, $t_s$ and $t_c$ respectively, below which a rule will be rejected for lack of evidence.
We denote the set rules that are learned and included as part of the final specification are as follows:

\begin{align}
Learn(\trainingSet) = \{r | \ &support(r) > t_s\ \land \nonumber \\
    & \textit{confidence(r)} > t_c\ \land \label{eq:learn}\\ 
    & S,\ T \subseteq \keys\} \nonumber
\end{align}

\subsection{Error Classes}
Each class of error forms a rule with a predicate $p$, and choices of $|S|$ and $|T|$, the sizes of the source and target keyword lists.
Ordering errors require $|S|=1, |T|=1$ and use the predicate $order$ which means if both keywords $S_0,T_0$ appear in a file, $S_0$ must come before $T_0$.
For example, the ordering error given in Sec.~\ref{sec:motiv} is expressed:
%
\begin{align*}
[\texttt{innodb\_data\_home\_dir}&] \in \validSet \implies \\
&\validSet \vdash order([\texttt{innoddb\_data\_home\_dir}],[\texttt{innodb\_data\_file\_path}])
\end{align*}
%
Missing keyword entry errors require $|S|, |T| = 1$ and use the predicate $missing$ to mean the keyword $T_0$ must appear, in any location, in the same file as the keyword $S_0$ in any configuration file.
The type rule is a set of rules over multiple predicates, where $|S|=1, |T|=1$.
These type rules take the form $S_0 \in \validSet \implies \validSet \vdash is\_([S_0],[S_0])$ where $\_$ matches all the basic types (bool, int, size, etc), as shown in Fig.~\ref{fig:allrules}.
In this case, the source and target are the same keyword, since a type constraint is only on a single keyword.

\app also supports two types of integer correlation rules, coarse-grained and fine-grained.
Both integer correlation rules are set of rules over the set of predicates $compare=\{<,=,>\}$.
Coarse grain rules require $|S|, |T| = 1$, and the predicates have the typical interpretation.
Fine-grained rules use $|S|=2,|T|=1$, and interpret the predicates such that for $k_1,\ k_2 \in S,\ k1*k2$ must have the predicate relation to $T_0$.
The integer correlation rules also use probabilistic types to prune the search space.
To avoid learning too many false positives, we restrict this rule to either $size*int=size$, or $int*int = int$, as shown in Fig.~\ref{fig:allrules}.
Without probabilistic typing, we would also learn rules which do not have a valid sematinc interpretation, for example a relation between three size keywords ($k1,k2,k3::size$) of the form $k1*k2>k3$.

{
\setlength{\abovecaptionskip}{-.05pt}
\setlength{\belowcaptionskip}{-15pt}
\begin{figure}
\begin{mathpar}
%\hspace{-1cm} %no idea why, but this is better than \centering here
\inferrule* [Right=bool]
{k_1 :: bool}
{isbool([k_1],[k_1]) :: Rule}
\and\quad
\inferrule* [Right=int]
{k_{1} :: int}
{isint([k_1],[k_1]) :: Rule}
\and\\
\inferrule* [Right=missing]
{ }
{missing([k_1],[k_2]) :: Rule}
\and\quad
\inferrule* [Right=coarse\_grain]
{k_1, k_2 :: int}
{compare([k_1],[k_2]) :: Rule}
\and\\
\inferrule* [Right=fine\_grain,before=\hspace{-1.3cm}]
{k_1,k_2,k_3 :: int}
{compare([k_1,k_2],[k_3]) :: Rule}
\and\quad
\inferrule* [Right=fine\_grain]
{k_1,k_3 :: size \\ k_2 :: int}
{compare([k_1,k_2],[k_3]) :: Rule}
\end{mathpar}
\caption{An expanded set of typing judgements for valid rules}
\label{fig:allrules}
\end{figure}
}

\subsection{Checker}
\label{sec-checker}

With the rules generated by the learner module, \app checks whether any entry in a target configuration file violates the learned rules and constraints.
\app parses a single verification target configuration file with the translator from Sec.~\ref{sec:trans}  to obtain a set of key-value pairs, $\mathcal{C}$, for that file.
Then, the checker applies the learner from Eq.~\ref{eq:learn} with $\textit{Learn}(\mathcal{C})$,
  to build the set of relations observed in the file, with the thresholds $t_s,t_c = 100\%$. 
The checker will then report the following set of errors:

\begin{align*}
\textit{Errors}(\mathcal{C}) = \{ r\ |\ & S_r \cup T_r \in \mathcal{C} \ \land \\
                               & r \in \textit{Learn}(\trainingSet) \ \land \\
                               & r \notin \textit{Learn}(\mathcal{C})
\end{align*}

For any relation from the verification target that violates a known rule, the checker will report the predicate and keyword sets associated with that rule as an error.
Since this is a probabilistic approach, in our tool \app, we provide the user with the support and confidence values as well to help the user determine if the rule must be satisfied in their particular system.
For instance, the \texttt{key\_buffer} misconfiguration from Sec. \ref{ex:fine} will only be noticeable if the system experiences a heavy traffic load, so the user may choose to ignore this error if they are confident this will not be an issue.

\iffalse
\subsection{Implementation}

\app is primarily implemented in Haskell.
The source code for our implementation is available at {\em (URL omitted for blind review)}.
We have developed \app to be easily extendable and customizable. Each of the classes of rules are implemented as an instance of the \textit{Learnable} interface (a typeclass in Haskell):

\begin{lstlisting}[language=Haskell, xleftmargin=.01\textwidth]
class (Eq a, Show a, Ord a, Countable b) => Learnable a b where
  buildRelations :: IRConfigFile -> RuleDataMap a b
  merge :: Countable b => [RuleDataMap a b] -> RuleDataMap a b
  check :: a -> b -> b -> Maybe b
  toError :: FilePath -> (a, b) -> Error
\end{lstlisting} 

Each of the rules that check for a particular type of error are implemented in three functions;
   \textit{buildRelations} for constructing a map of possible observations to a count of how often they are observed, 
   \textit{merge} function for taking multiple sets of built relations from different files and combining them into one set of relations (weighted accordingly), 
   and finally, a \textit{check} that takes an individual relation mapping from our built set of relations, the observation in the file we are verifying, and whether there is a violation of that particular observation of the relation. 
(There is a fourth \textit{toError} function that serves as a helper function to pretty-print the output of \app.) 
Hence, if we can define how to build the relations to verify against a new type of error, a system for combining these relations with weights across multiple files that are seen, along with a mechanism for checking whether each individual relation was violated or not, we can extend \app to check for new types of errors. Furthermore, the approach of defining a method to build relations from a single file as well as a method to combine these relations facilitates the parallelization of our implementation, since each of the \textit{buildRelations} calls can work parlellely on each file before the results are combined with the \textit{merge} step.

Each of the outputs of the \textit{buildRelations} is in the form \textit{RuleDataMap a b}, where \textit{a} is a group of entities between which we want to define a relation (for example, a pair of keywords for Missing, Order, and Integer Correlation) and \textit{Countable b} is a tally of likelihoods of each possible outcome of the relation that can be weighted and combined over multiple files (for example, a count of files that exhibit the relation versus not for Missing or a count of files that exhibit less than, equals, or greater than between a pair of keywords for Ordering or a set of probabilities for Type rules).

Once the program has finished merging together a set of built relations over every example file in the learning set, we have implemented a \textit{check} function for each of the rules, where we input the group of entities under which a relation is defined, the expected observation on that relation, and the actually observed observation in the file we are checking. If the observed instance is inconsistent with the relations built from our training set files, then we would see it reported. In the probablistic setting of our tool, we would implement cutoffs to tune our \textit{check} to ignore observed relations under a certain significance level.
\fi
