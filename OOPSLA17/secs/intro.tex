\section{Introduction}
\label{sec-intro}

%what is the problem
Configuration errors (also known as misconfigurations) have become
one of the major causes of system failures, resulting in security vulnerabilities,
application outages, and incorrect program executions~\cite{xu15systems, xu13do, xu15hey}. 
In a recent high visible case~\cite{mashableNews},  
Facebook and Instagram became inaccessible
and a Facebook spokeswoman reported that 
this was caused by a change to the site's configuration systems.
In another case, a system configuration change caused an outage of AT\&T's 911 service for five hours.
During this time, 12,600 unique callers were not able to reach the 911 emergency line~\cite{att-outage}.
These critical system failures are not rare -
a software system failures study~\cite{yin11anempirical},
reports that about 31\% of system failures were caused by 
configuration errors.
This is even higher than the percentage of failures resulting from program bugs (20\%).

%what has been done
The systems research community has recognized this as an important
problem and many efforts have been proposed to
check, troubleshoot, and diagnose configuration 
errors~\cite{attariyan10automating,
su07autobash, whitaker04configuration,xu16early}.
However, these tools rely on analyzing the source code of 
the target systems or need to run the system
multiple times to understand the source of the errors.
The support for a static analysis style verification for configuration files
is still not on the level of
automated verification tools used for regular program 
verification~\cite{Leino10Dafny, PiskacWZ14, BobotFMP15} that can
preemptively detect errors. 

%why is that insufficent
%\markk{xu16early should go here somehow}
There have been several prior efforts that attempt to proactively verify 
configuration files~\cite{santolucitoCAV, xu16early,
zhang14encore, huang15confvalley}.
However, state-of-the-art efforts have limitations that are impractical to meet in practice.
In general, these efforts fall into two categories with respect to their limitations.
\begin{itemize}
\item On the one hand there are tools that can detect sophisticated 
configuration errors, \eg, ConfigC~\cite{santolucitoCAV}. 
However, these tools heavily rely on datasets containing 100\% 
correct configuration files to extract configuration rules.
Existing investigation studies~\cite{wang04automatic, yin11anempirical}
have demonstrated determining or obtaining 100\% correct configuration
files to drive rules is almost impossible in reality. 
Without 100\% correct datasets, these tools do not work.
\item On the other hand there are tools, 
\eg, EnCore~\cite{zhang14encore} and
ConfValley~\cite{huang15confvalley}, that can only 
detect rather simplistic configuration errors (\eg, value range errors 
and simple integer correlation errors), but cannot detect
more complex configuration errors, such as ordering errors, fine-grain correlations, missing entry errors.
\end{itemize}

%what do we do
In this paper, we propose a framework for automated verification of 
configuration files, which mimics standard automated verification 
techniques: for a given configuration file, we  checked if it 
adheres to a specification, and if there are issues we report them to 
the user.

% how did we do it
There are two main obstacles to directly applying existing automatic program
verification techniques to configuration files.
First, a lack of specifications on the properties of configuration files makes the verification task poorly defined.
There are surprisingly few rules specifying constraints on entries, even written in plain English.
Since asking users to write an entire specification for configuration files is impractical and error-prone, we instead take a specification generation approach.
Second, the flat structure of configuration files is only a sequence of entries assigning some value to system
variables (called {\emph {keywords}}) and provides little structural information.
In particular, the keywords in configuration files are often untyped, a useful property to leverage in program verification.

Our main task was to first derive a specification for configuration 
files. We first process a \ruzica{mention the size, size does matter :)}training set of configuration files. Those files are benchmarks 
available on-line~\ruzica{put a citation here}. They are mostly correct
but they also may contain some errors.

In order to learn a specification from this large set of configuration
files,
the first step is to 
translate the training set into a more structured typed representation.
We then apply the learning process and we learn an abundant set of rules 
specifying various properties that hold on the given training set. 
\ruzica{say here 2 sentences about the learning process }

The rules, in general, 
specify which properties keywords in configuration files need to satisfy.
The learning process is language-agnostic and works for any kind 
of configuration files, though all of the files in the training set need to be of 
the same language (such as MySQL or HTTPD configuration files).

This learning process is a way to derive a specification for configuration files. 
With the learned specification, \app can then efficiently check 
the correctness of the configuration files of interest and detect 
potential errors. Errors are reported if the configuration file does not 
adhere to the derived specification.

The errors that we learn are detailed and complex enough proactively report a wide range of potential misconfigurations.
In this framework, the example configurations from which we derive the specifications do not need to be labeled as correct or incorrect.







The errors \app detects may cause total system failures, but can also be more insidious, for example slowing down the system only when the server load increases beyond a certain threshold.
Since these runtime errors may only be triggered after some time in a deployment environment, the standard debugging 
technique~\cite{Zeller:2005:WPF:1077048} of starting a system multiple times with different configuration settings will not help detect these misconfigurations.

%did what we did work?
Our learning algorithm surmounts both of these shortcomings present in 
existing tools.  \app implements a learning
algorithm inspired by {\em association rule
learning}~\cite{agrawal1993mining}. It analyzes a large training set of configuration files~\cite{configdataset}. Those are real-world reported misconfigurations. 
A file in the training set might contain several different errors and these errors only appear 
in a small percentage of files. 
We first translate those file into an intermediary typed language. 
With with every type, we associate a set of very general interfaces. 
The user does not need to provide them - they are internally associated with the types. 
The learner then instantiates those interfaces with the keywords appearing in 
the training set. However, since files might also contain errors we take 
this into account when learning correct rules. All learned rules are 
annotated with the probability of correctness.  
To ensure the learned rules are correct enough,
\app employs a graph analysis to refine the set learned rules.
\app analyzes the rule graph to rank the importance and relevance of the learned rules. 

We have implemented a tool, \app, and evaluated it on almost 1000 real-
world configuration files from Github.
We demonstrate that we are able to detect known errors, based on 
StackOverflow posts, in this data set.
Furthermore, we find compelling evidence that out optimizations are 
effective, for example using probabilistic types removes 1023 false 
positives errors reports to reduce the total error reports to 324.
The rule graph analysis drastically improves the ranking of importance of 
the error reports, which allows users to more quickly fix the most 
critical misconfigurations.
Additionally, \app scales linearly, whereas 
a previous tool~\cite{santolucitoCAV} showed exponential slow downs on 
the same benchmark set.

In summary, we make the following contributions:
\begin{itemize}
\item We propose the automated configuration verification
framework, \app, that can learn specifications from a training set
of configuration files, and then use the specifications to verify 
configuration files of interest.
\item We describe the logical foundation of using association rule learning to build a probabilistic specification for configuration files. 
\item We analyze the learned rules to further refine the generated specification and we empirically show the usefulness of this approach.
\item We implement a \app prototype and evaluate it by
detecting sophisticated configuration errors 
from real-world dataset.

\end{itemize}


\com{ 


% EnCore also used association rule learning, but did not describe why it is useful for verification in particular,
%   or how it contrasts with other ML techniques that are less appropriate for verification (NN).
\item Describing the logical foundation of using association rule learning to build a probabilistic specification for configuration files. 
      We contrast this with the application of other machine learning techniques, such as neural networks, in verification.

% We can analyze the rules we have learned as a graph to sort the errors, not just by weight, but also importance
% need one more thing we can do with this graph
% This is only possible when we have learned association rules, not possible with NN
\item Analyzing the learned model to further refine the generated specification. 
      We highlight why these analysis techniques are uniquely possible in an association rule learning approach.

% The problem is intractable for off-the-shelf data mining solutions (see EnCore).
% In addition to the optimizations EnCore used, we add new ones that make our tool even better.  
\item An open-source implementation, \app, of this approach, which uses various domain specific optimizations to make the solution efficient. 
      We also evaluate \app on a set of 256 real world benchmarks, and identify XX new misconfigurations errors on YY of these files.
\end{enumerate}




%Therefore, automated verification of configuration 
%files would be highly
%desirable~\cite{wang04automatic, zhang14encore, xu15systems}.

%Offering automatic verification to configuration files -- like
%what we did to programs -- has been advocated as a reasonable means
%to check the correctness of configuration files of interest.
%Nevertheless, it is still an open problem, because 
%1) software configurations are typically written in poorly structured 
%and untyped languages, and 2) writing specifications or constraints
%for configuration verification is non-trivial in practice.


For practical purposes we use a real-world dataset~\cite{configdataset}. 
Every file in our dataset contains several errors, 
but they are typically different errors and only appear 
in a small percentage of files. Using that insight we were able, with the help of the probabilistic cutoff, to learn an
accurate set of rules. The rules, in general, specify which properties variables need to satisfy. One can see this learning process as a
way of deriving  
a specification for configuration files.  Once there is a specification, we can do formal verification. With these 
rules we can efficiently check the correctness of the configuration files
of interest and detect potential errors.

From a practical perspective, 
\app introduces no additional burden 
to the user: they can simply use \app to check for errors in their
configuration files. However, they can also easily extend the framework
themselves. The system is designed to be highly modular. If there is a
class of rules that \app is not currently learning, the user can develop
her own templates and learners for that class. The new learner can be
added to \app and this way it can check an additional new set of
errors.

\begin{enumerate}

\item We propose a configuration verification
framework, \app, that can learn a language model from a sample
dataset, and then use this language model to verify 
configuration files of interest.
 
\item \app proposes probabilistic types to assign a confidence 
distribution over a set of types to each entry, 
while generating the intermediate representation. 

\item \app employs a collection of machine learning algorithms to 
enable powerful rule and constraint inference.

\item \app is capable of detecting various tricky errors that cannot
be detected by previous efforts,
including entry ordering errors, fine-grained value correlation errors, 
missing entry errors, and environment-related errors. 

\end{enumerate}

}
