\section{Introduction}
\label{sec-intro}

Configuration errors (also known as misconfigurations) have become
one of the major causes of system failures, resulting in security vulnerabilities,
application outages, and incorrect program executions~\cite{xu15systems, xu13do, xu15hey}. Recently, there was a problem in accessing  
Facebook and Instagram~\cite{mashableNews}, 
and a Facebook spokeswoman reported that 
this was caused by a change to the site's configuration systems.
\ruzica{1) one more such example? we have enough space. 2) do something about citations (mas 2015) looks silly 3) there is no comparison, or even
mentioning of the OSDI paper 2016, which is on configuration files and they got even the best paper award }
In a software system failures study~\cite{yin11anempirical},
it was shown that about 31\% of system failures were caused by 
configuration errors, which is higher than the percentage of
failures resulting from program bugs (20\%).

In this paper, we propose
a framework for automated verification of configuration files: 
we derive specifications by learning ``raw'' configuration files 
and proactively report potential errors in the configuration files
of interest, without waiting for them to happen. 
We developed a tool, called \app, 
and evaluated it on 
almost a thousand configuration files from Github. The systems research community has recognized this as an important
problem~\cite{xu16early}. While many efforts have been proposed to
check, troubleshoot, and diagnose configuration 
errors~\cite{attariyan10automating,
su07autobash, whitaker04configuration},
those tools heavily rely on analyzing the source code of 
the target systems and need to run these systems
multiple times to understand where are the errors---\ie,
they are still not on a level of
automated verification tools used for regular program 
verification~\cite{Leino10Dafny, PiskacWZ14, BobotFMP15} that can
detect errors without executing the code. 

We believe there are two main obstacles 
to why we cannot simply apply the existing automatic program
verification 
techniques to verification of configuration files:
1) the lack
of a specification describing properties of configuration files;
2) the structure of configuration files -- they
are mainly a sequence of entries assigning some value to system
variables (called {\emph {keywords}}). 
The language in which configuration files are written does 
not adhere to a specific grammar or syntax. In particular, the
entries in configuration files are untyped. Moreover, there are surprisingly few rules specifying constraints on entries, and there
is no explicit structure policy for the entries.

\app overcomes the above obstacles by first automatically inferring a
specification for configuration files. It is unrealistic to expect the 
users to write an entire specification for configuration files on their own. 
This process can easily lead to incomplete or even contradictory 
specifications. Instead, we learn specification from 
a large training set of 
configuration files~\cite{configdataset}. The first step is to 
translate this training set into a more structured typed representation.
We then apply the learning process and we learn an abundant set of rules 
specifying various properties that hold on the given training set. The rules, in general, 
specify which properties keywords in configuration files need to satisfy.
The learning process is language-agnostic and works for any kind 
of configuration files,  but all of the files in the training set need to be of 
the same kind (such as MySQL or HTTPD configuration files).
We see this learning process as 
a way of deriving  a specification for configuration files. 
It is hard to talk here about a complete specification, but it is still
a set of formal rules describing the properties that configuration files need to satisfy. 

Having a specification, \app can then efficiently check 
the correctness of the configuration files of interest and detect 
potential errors. Errors are reported if the configuration file does not 
adhere to the derived specification.

The errors found can cause total system failures, but can also be more insidious, for example slowing down the system only when the server load increases beyond a certain threshold.
Since these runtime errors may only be triggered after some time in a deployment environment, the standard debugging 
techniques~\cite{Zeller:2005:WPF:1077048} of starting a system multiple times with different configuration settings will not help detect these misconfigurations.

To the best of our knowledge, there have been several prior efforts that attempt to proactively verify 
configuration files~\cite{santolucitoCAV, xu16early,
zhang14encore, huang15confvalley}.
However, state-of-the-art efforts have limitations that are impractical to meet in practice.
In general, these efforts fall into two categories with respect to their limitations.\begin{itemize}
\item On the one hand there are tools that can detect sophisticated 
configuration errors, \eg, ConfigC~\cite{santolucitoCAV}. 
However, these tools heavily rely on datasets containing 100\% 
correct configuration files to extract configuration rules.
Existing investigation studies~\cite{wang04automatic, yin11anempirical}
have demonstrated determining or obtaining 100\% correct configuration
files to drive rules is almost impossible in reality. 
Without 100\% correct datasets, these tools do not work.
\item On the other hand there are tools, 
\eg, EnCore~\cite{zhang14encore} and
ConfValley~\cite{huang15confvalley}, that can only 
detect rather simplistic configuration errors (\eg, value range errors 
and simple integer correlation errors), but cannot detect
more complex configuration errors, such as ordering errors or 
Nevertheless, these types of errors cannot be detected by above existing
efforts.
\end{itemize}

Our learning algorithm surmounts both of these shortcomings present in 
existing tools.  \app implements a learning
algorithm inspired by {\em association rule
learning}~\cite{agrawal1993mining}. It analyzes a large training set of configuration files~\cite{configdataset}. Those are real-world reported misconfigurations. A file in the training set might 
contain several different errors and these errors only appear 
in a small percentage of files. We first translate those file into an 
intermediary typed language. With with every type, we associate 
a set of very general interfaces. The user does not
need to provide them - they are internally associated with the types. The 
learner then instantiates those interfaces with the keywords appearing in 
the training set. However, since files might also contain errors we take 
this into account when learning correct rules. All learned rules are 
annotated with the probability of correctness.  
To ensure the learned rules are correct enough,
\app employs a graph analysis to refine the set learned rules.
\app analyzes the rule graph to rank the importance and relevance of the learned rules. 

We have implemented a tool, \app, and evaluated it on almost 1000 real-
world configuration files from Github.
We demonstrate that we are able to detect known errors, based on 
StackOverflow posts, in this data set.
Furthermore, we find compelling evidence that out optimizations are 
effective, for example using probabilistic types removes 1023 false 
positives errors reports to reduce the total error reports to 324.
The rule graph analysis drastically improves the ranking of importance of 
the error reports, which allows users to more quickly fix the most 
critical misconfigurations.
Additionally, \app scales linearly, whereas 
a previous tool~\cite{santolucitoCAV} showed exponential slow downs on 
the same benchmark set.

In summary, we make the following contributions:
\begin{itemize}
\item We propose the automated configuration verification
framework, \app, that can learn specifications from a training set
of configuration files, and then use the specifications to verify 
configuration files of interest.
\item We describe the logical foundation of using association rule learning to build a probabilistic specification for configuration files. 
\item We analyze the learned rules to further refine the generated specification and we empirically show the usefulness of this approach.
\item We implement a \app prototype and evaluate it by
detecting sophisticated configuration errors 
from real-world dataset.

\end{itemize}


\com{ 


% Encore also used association rule learning, but did not describe why it is useful for verification in particular,
%   or how it contrasts with other ML techniques that are less appropriate for verification (NN).
\item Describing the logical foundation of using association rule learning to build a probabilistic specification for configuration files. 
      We contrast this with the application of other machine learning techniques, such as neural networks, in verification.

% We can analyze the rules we have learned as a graph to sort the errors, not just by weight, but also importance
% need one more thing we can do with this graph
% This is only possible when we have learned association rules, not possible with NN
\item Analyzing the learned model to further refine the generated specification. 
      We highlight why these analysis techniques are uniquely possible in an association rule learning approach.

% The problem is intractable for off-the-shelf data mining solutions (see Encore).
% In addition to the optimizations Encore used, we add new ones that make our tool even better.  
\item An open-source implementation, \app, of this approach, which uses various domain specific optimizations to make the solution efficient. 
      We also evaluate \app on a set of 256 real world benchmarks, and identify XX new misconfigurations errors on YY of these files.
\end{enumerate}




%Therefore, automated verification of configuration 
%files would be highly
%desirable~\cite{wang04automatic, zhang14encore, xu15systems}.

%Offering automatic verification to configuration files -- like
%what we did to programs -- has been advocated as a reasonable means
%to check the correctness of configuration files of interest.
%Nevertheless, it is still an open problem, because 
%1) software configurations are typically written in poorly structured 
%and untyped languages, and 2) writing specifications or constraints
%for configuration verification is non-trivial in practice.


For practical purposes we use a real-world dataset~\cite{configdataset}. 
Every file in our dataset contains several errors, 
but they are typically different errors and only appear 
in a small percentage of files. Using that insight we were able, with the help of the probabilistic cutoff, to learn an
accurate set of rules. The rules, in general, specify which properties variables need to satisfy. One can see this learning process as a
way of deriving  
a specification for configuration files.  Once there is a specification, we can do formal verification. With these 
rules we can efficiently check the correctness of the configuration files
of interest and detect potential errors.

From a practical perspective, 
\app introduces no additional burden 
to the user: they can simply use \app to check for errors in their
configuration files. However, they can also easily extend the framework
themselves. The system is designed to be highly modular. If there is a
class of rules that \app is not currently learning, the user can develop
her own templates and learners for that class. The new learner can be
added to \app and this way it can check an additional new set of
errors.

\begin{enumerate}

\item We propose a configuration verification
framework, \app, that can learn a language model from a sample
dataset, and then use this language model to verify 
configuration files of interest.
 
\item \app proposes probabilistic types to assign a confidence 
distribution over a set of types to each entry, 
while generating the intermediate representation. 

\item \app employs a collection of machine learning algorithms to 
enable powerful rule and constraint inference.

\item \app is capable of detecting various tricky errors that cannot
be detected by previous efforts,
including entry ordering errors, fine-grained value correlation errors, 
missing entry errors, and environment-related errors. 

\end{enumerate}

}
