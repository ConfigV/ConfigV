\section{Introduction}
\label{sec-intro}

Configuration errors are one of the most important root causes of today's 
software system failures~\cite{xu15systems, yin11anempirical}.
In their empirical study, Yin {\em et al.}~\cite{yin11anempirical} report 
that about 31\% of system failures were caused by misconfiguration problems
and only 20\% were caused by bugs in program code. 
Misconfigurations, in practice, may result in various system-wide problems,
including security vulnerabilities, application crashes, severe disruptions
in software functionality, 
and incorrect program executions~\cite{xu15systems, xu13do, xu15hey}.  
Therefore, how to automatically verify configuration files is an open
problem~\cite{xu15systems}.

To the best of our knowledge, there have been several prior 
efforts that attempt to verify 
configuration files~\cite{santolucitoCAV, xu16early,
zhang14encore, huang15confvalley}.
However, state-of-the-art efforts either are impractical to be used
in reality, or can only deal with simplistic configuration errors.
In general, these efforts fall into two categories.

\squishlist

\item On the one hand are tools that can detect sophisticated 
configuration errors, \eg, ConfigC~\cite{santolucitoCAV}. 
However, these tools heavily rely on datasets containing 100\% 
correct configuration files to extract configuration rules.
Existing investigation studies~\cite{wang04automatic, yin11anempirical}
have demonstrated determining or obtaining 100\% correct configuration
files to drive rules is almost impossible in reality. 
Without 100\% correct datasets, these tools would, unfortunately, 
work poorly.

\item On the other hand, practical efforts, 
\eg, EnCore~\cite{zhang14encore} and
ConfValley~\cite{huang15confvalley}, can only 
detect simplistic configuration errors (\eg, value range errors 
and simple integer correlation errors), but cannot detect
sophisticated configuration errors. 
Consider entry ordering errors, 
which happen when some entries are inserted in a wrong order
in the configuration files. 
For example, in a PHP configuration file, 
an entry \code{extension=mysql.so} appears 
before \code{extension=recode.so},  
it would cause Apache server to crash. The correct ordering 
should be \code{extension=recode.so} before 
\code{extension=mysql.so}~\cite{yin11anempirical}.
Nevertheless, these types of errors cannot be detected by above existing
efforts.

\squishend

The goal of this work is to propose and build a {\em practical} 
configuration
verification tool capable of checking {\em sophisticated} 
configuration errors (more details
for sophisticated configuration errors in Sec.~\ref{sec:motiv}).
Achieving this goal requires us addressing two main obstacles:
1) the lack of a specification describing properties of configuration
files, and 2) the structure of configuration files---they are 
mainly a sequence of entries assigning some values to system
parameters (called keywords).
In other words, the ``language'' in which configuration 
files are written does not adhere to any specific grammar or syntax. 
Moreover, the entries in configuration files 
lack type or rules specifying constraints on entries, 
and there is no explicit structure policy for these entries.

This paper presents \app, the first automatic verification framework
for general software configurations, by overcoming the above two
obstacles. In general, \app verifies a given configuration file $F$ 
through three steps:

\para{Step~1: Translator}
\app takes as input a large training set of configuration files
belonging the same system as $F$,
and we do not have any assumption on the correctness of these files.
\app analyzes the training set, and generates a well-structured and
probabilistically-typed intermediate representations.

\para{Step~2: Learning}
\app derives configuration rules by running {\em rule association
learning} algorithm to learn the generated intermediate representations.
The insight of our design is: every file in the training set
contains several different errors and these errors only appear 
in a small percentage of files. Based on this insight,
we are able, with the help of the probabilistic cutoff, to learn a
set of configuration rules. These rules, in general, specify 
which properties the entries in configuration files need to satisfy. 
One can see this learning process as a way of deriving  
a specification for configuration files.  
Once a specification is obtained, we can do verification that 
uses these learned rules to check the correctness of 
the configuration files
of interest and detect potential errors.

\para{Step~3: Rule graph analysis}
To ensure the learned rules are good enough,
\app employs a further analysis stage to refine the learned rules.
In general, \app builds a graph to model the learned rules,
and analyzes this graph's properties, ranking the importance
and relevance of these rules. Finally, \app outputs
a set of refined rules as the eventual specification
to check the target configuration file $F$.

We have implemented a \app prototype, and evaluated it based on
real-world datasets. In our verification experiments, 
we first use a public configuration dataset~\cite{configdataset}, which
contains 261 incorrect MySQL configuration files, as our
training set. We then use \app to learn rules from this training set
and refine these rules. Finally, we check sophisticated 
errors of real configuration files crawled from github,
and detect many errors in these widely-used configuration files.

In summary, we make the following contributions.


\com{ 


\markk{maybe we can use these lines in the intro somewhere.}
These errors can cause total system failures, but can also be more insidious, for example slowing down the system only when the server load increases beyond a certain threshold.
Since these runtime errors may only be triggered after some time in a deployment environment, the standard delta-debugging technique~\cite{x} of starting a system multiple times with different configuration settings will not help detect these misconfigurations.

Our main contributions are as follows:

\begin{enumerate}

% Encore also used association rule learning, but did not describe why it is useful for verification in particular,
%   or how it contrasts with other ML techniques that are less appropriate for verification (NN).
\item Describing the logical foundation of using association rule learning to build a probabilistic specification for configuration files. 
      We contrast this with the application of other machine learning techniques, such as neural networks, in verification.

% We can analyze the rules we have learned as a graph to sort the errors, not just by weight, but also importance
% need one more thing we can do with this graph
% This is only possible when we have learned association rules, not possible with NN
\item Analyzing the learned model to further refine the generated specification. 
      We highlight why these analysis techniques are uniquely possible in an association rule learning approach.

% The problem is intractable for off-the-shelf data mining solutions (see Encore).
% In addition to the optimizations Encore used, we add new ones that make our tool even better.  
\item An open-source implementation, \app, of this approach, which uses various domain specific optimizations to make the solution efficient. 
      We also evaluate \app on a set of 256 real world benchmarks, and identify XX new misconfigurations errors on YY of these files.
\end{enumerate}




%Therefore, automated verification of configuration 
%files would be highly
%desirable~\cite{wang04automatic, zhang14encore, xu15systems}.

%Offering automatic verification to configuration files -- like
%what we did to programs -- has been advocated as a reasonable means
%to check the correctness of configuration files of interest.
%Nevertheless, it is still an open problem, because 
%1) software configurations are typically written in poorly structured 
%and untyped languages, and 2) writing specifications or constraints
%for configuration verification is non-trivial in practice.


For practical purposes we use a real-world dataset~\cite{configdataset}. 
Every file in our dataset contains several errors, 
but they are typically different errors and only appear 
in a small percentage of files. Using that insight we were able, with the help of the probabilistic cutoff, to learn an
accurate set of rules. The rules, in general, specify which properties variables need to satisfy. One can see this learning process as a
way of deriving  
a specification for configuration files.  Once there is a specification, we can do formal verification. With these 
rules we can efficiently check the correctness of the configuration files
of interest and detect potential errors.

From a practical perspective, 
\app introduces no additional burden 
to the user: they can simply use \app to check for errors in their
configuration files. However, they can also easily extend the framework
themselves. The system is designed to be highly modular. If there is a
class of rules that \app is not currently learning, the user can develop
her own templates and learners for that class. The new learner can be
added to \app and this way it can check an additional new set of
errors.

\begin{enumerate}

\item We propose the first automatic configuration verification
framework, \app, that can learn a language model from a sample
dataset, and then use this language model to verify 
configuration files of interest.
 
\item \app proposes probabilistic types to assign a confidence 
distribution over a set of types to each entry, 
while generating the intermediate representation. 

\item \app employs a collection of machine learning algorithms to 
enable powerful rule and constraint inference.

\item \app is capable of detecting various tricky errors that cannot
be detected by previous efforts,
including entry ordering errors, fine-grained value correlation errors, 
missing entry errors, and environment-related errors. 

\item We implement a \app prototype and evaluate it by
conducting comprehensive experiments on real-world dataset.

\end{enumerate}

}
