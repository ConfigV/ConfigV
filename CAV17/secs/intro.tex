\section{Introduction}
\label{sec-intro}

Configuration errors (also known as misconfigurations) have become
one of the major causes of system failures, resulting in security vulnerabilities,
application outages, and incorrect program executions~\cite{xu15systems, xu13do, xu15hey}. Recently, there was a problem in accessing  
Facebook and Instagram~\cite{mashableNews}, 
and a Facebook spokeswoman reported that 
this was caused by a change to the site's configuration systems.
In a recent software system failures study~\cite{yin11anempirical},
researchers revealed that about 31\% of system failures were caused by 
configuration errors, which is higher than the percentage of
failures resulting from program bugs (20\%).

The systems research community has recognized this as an important
problem~\cite{xu16early}. While many efforts have been proposed to check, troubleshoot, diagnose, and repair configuration 
errors~\cite{attariyan10automating,
su07autobash, whitaker04configuration},
those tools mainly try to understand {\emph{what}} caused the 
error -- they are still not on a level of
automatic verification tools used for regular program 
verification~\cite{Leino10Dafny, PiskacWZ14, BobotFMP15} that can
detect errors without executing the code. In this paper we propose
a framework for automated verification of configuration files: we are analyzing 
files and proactively reporting potential errors, without waiting for them to happen. We developed a tool, called \app, and tested it on 
almost a thousand file from github.

We believe there are two main obstacles 
to why we cannot simply apply  the existing automatic 
tools and techniques to verification of configuration files:
1) the lack
of a specification describing properties of configuration files;
2) the structure of configuration files -- they
are mainly a sequence of entries assigning some value to system
variables (called {\emph {keywords}}). 
The language in which configuration files are written does 
not adhere to a specific grammar or syntax. In particular, the
entries in configuration files are untyped. Moreover, there are surprisingly few rules specifying constraints on entries, and there
is no explicit structure policy for the entries.

\app overcomes the above obstacles by first automatically inferring a
specification for configuration files. It is unrealistic to expect the 
users to write an entire specification for configuration files on their own. 
This process can easily lead to incomplete or even contradictory 
specifications. Instead, we learn specification from 
a large sample of 
configuration files~\cite{configdataset}. The first step is to 
translate this training set into a more structured typed representation.
We then apply the learning process and we learn an abundant set of rules 
specifying various properties that hold on the given sample. The rules, in general, 
specify which properties keywords in configuration files need to satisfy.
The learning process is language-agnostic and works for any kind 
of configuration files,  but all of the files in the sample need to be of 
the same kind (such as MySQL or HTTPD configuration files).
We see this learning process as 
a way of deriving  a specification for configuration files. 
It is hard to talk here about a complete specification, but it is still
a set of formal rules describing the properties that configuration files need to satisfy. 

Having a specification, \app can then efficiently check 
the correctness of the configuration files of interest and detect 
potential errors. Errors are reported if the configuration file does not 
adhere to the derived specification.

There have been several prior efforts that attempt to verify 
configuration files~\cite{santolucitoCAV, xu16early,
zhang14encore, huang15confvalley}.
However, state-of-the-art efforts either are impractical to be used
in reality, or can only deal with simplistic configuration errors.
In general, these efforts fall into two categories.\begin{itemize}
\item On the one hand there are tools that can detect sophisticated 
configuration errors, \eg, ConfigC~\cite{santolucitoCAV}. 
However, these tools heavily rely on datasets containing 100\% 
correct configuration files to extract configuration rules.
Existing investigation studies~\cite{wang04automatic, yin11anempirical}
have demonstrated determining or obtaining 100\% correct configuration
files to drive rules is almost impossible in reality. 
Without 100\% correct datasets, these tools do not work.
\item On the other hand there are tools, 
\eg, EnCore~\cite{zhang14encore} and
ConfValley~\cite{huang15confvalley}, that can only 
detect rather simplistic configuration errors (\eg, value range errors 
and simple integer correlation errors), but cannot detect
more complex configuration errors, such as ordering errors or 
Nevertheless, these types of errors cannot be detected by above existing
efforts.
\end{itemize}

Our learning algorithm surmounts all of difficulties present in 
existing tools.  \app implements a learning
algorithm inspired by {\em rule association
learning}~\cite{agrawal1993mining}. It analyzes a large training set of configuration  files~\cite{configdataset}. Those are real-world reported misconfigurations. A file in the training set might 
contain several different errors and these errors only appear 
in a small percentage of files. We first translate those file into an 
intermediary typed language. With with every type we associate 
a set of very general templates. The user does not
need to provide any templates, they are internally associated to the types. 


 algorithm to learn the generated intermediate representations.
The insight of our design is: every file in the training set
contains several different errors and these errors only appear 
in a small percentage of files. Based on this insight,
we are able, with the help of the probabilistic cutoff, to learn a
set of configuration rules. These rules, in general, specify 
which properties the entries in configuration files need to satisfy. 
One can see this learning process as a way of deriving  
a specification for configuration files.  
Once a specification is obtained, we can do verification that 
uses these learned rules to check the correctness of 
the configuration files
of interest and detect potential errors.





\para{Step~3: Rule graph analysis}
To ensure the learned rules are good enough,
\app employs a further analysis stage to refine the learned rules.
In general, \app builds a graph to model the learned rules,
and analyzes this graph's properties, ranking the importance
and relevance of these rules. Finally, \app outputs
a set of refined rules as the eventual specification
to check the target configuration file $F$.

We have implemented a \app prototype, and evaluated it based on
real-world datasets. In our verification experiments, 
we first use a public configuration dataset~\cite{configdataset}, which
contains 261 incorrect MySQL configuration files, as our
training set. We then use \app to learn rules from this training set
and refine these rules. Finally, we check sophisticated 
errors of real configuration files crawled from github,
and detect many errors in these widely-used configuration files.

In summary, we make the following contributions.


\com{ 


\markk{maybe we can use these lines in the intro somewhere.}
These errors can cause total system failures, but can also be more insidious, for example slowing down the system only when the server load increases beyond a certain threshold.
Since these runtime errors may only be triggered after some time in a deployment environment, the standard delta-debugging technique~\cite{x} of starting a system multiple times with different configuration settings will not help detect these misconfigurations.

Our main contributions are as follows:

\begin{enumerate}

% Encore also used association rule learning, but did not describe why it is useful for verification in particular,
%   or how it contrasts with other ML techniques that are less appropriate for verification (NN).
\item Describing the logical foundation of using association rule learning to build a probabilistic specification for configuration files. 
      We contrast this with the application of other machine learning techniques, such as neural networks, in verification.

% We can analyze the rules we have learned as a graph to sort the errors, not just by weight, but also importance
% need one more thing we can do with this graph
% This is only possible when we have learned association rules, not possible with NN
\item Analyzing the learned model to further refine the generated specification. 
      We highlight why these analysis techniques are uniquely possible in an association rule learning approach.

% The problem is intractable for off-the-shelf data mining solutions (see Encore).
% In addition to the optimizations Encore used, we add new ones that make our tool even better.  
\item An open-source implementation, \app, of this approach, which uses various domain specific optimizations to make the solution efficient. 
      We also evaluate \app on a set of 256 real world benchmarks, and identify XX new misconfigurations errors on YY of these files.
\end{enumerate}




%Therefore, automated verification of configuration 
%files would be highly
%desirable~\cite{wang04automatic, zhang14encore, xu15systems}.

%Offering automatic verification to configuration files -- like
%what we did to programs -- has been advocated as a reasonable means
%to check the correctness of configuration files of interest.
%Nevertheless, it is still an open problem, because 
%1) software configurations are typically written in poorly structured 
%and untyped languages, and 2) writing specifications or constraints
%for configuration verification is non-trivial in practice.


For practical purposes we use a real-world dataset~\cite{configdataset}. 
Every file in our dataset contains several errors, 
but they are typically different errors and only appear 
in a small percentage of files. Using that insight we were able, with the help of the probabilistic cutoff, to learn an
accurate set of rules. The rules, in general, specify which properties variables need to satisfy. One can see this learning process as a
way of deriving  
a specification for configuration files.  Once there is a specification, we can do formal verification. With these 
rules we can efficiently check the correctness of the configuration files
of interest and detect potential errors.

From a practical perspective, 
\app introduces no additional burden 
to the user: they can simply use \app to check for errors in their
configuration files. However, they can also easily extend the framework
themselves. The system is designed to be highly modular. If there is a
class of rules that \app is not currently learning, the user can develop
her own templates and learners for that class. The new learner can be
added to \app and this way it can check an additional new set of
errors.

\begin{enumerate}

\item We propose the first automatic configuration verification
framework, \app, that can learn a language model from a sample
dataset, and then use this language model to verify 
configuration files of interest.
 
\item \app proposes probabilistic types to assign a confidence 
distribution over a set of types to each entry, 
while generating the intermediate representation. 

\item \app employs a collection of machine learning algorithms to 
enable powerful rule and constraint inference.

\item \app is capable of detecting various tricky errors that cannot
be detected by previous efforts,
including entry ordering errors, fine-grained value correlation errors, 
missing entry errors, and environment-related errors. 

\item We implement a \app prototype and evaluate it by
conducting comprehensive experiments on real-world dataset.

\end{enumerate}

}
