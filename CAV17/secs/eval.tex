
\section{Evaluation}

\markk{TODO rewrite all this}
We evaluate this approach using our implementation, \app.
We use two training sets, a preexisting set of MySQL configuration from~\cite{xu15systems} of 256 files,
 and our own set of collected MySQL configuration from Github of 973 files.
In our evaluation we focus on MySQL for consistency, but any configuration language (that can be parsed to the intermediate representation from Sec.~\ref{sec:trans}) can be used.

We run \app on 256 files from Tianyin, as a training set to generate rules.
We scraped github for 973 mysql config files as a test set.
We report the number of rules learned and number of errors detected in Table~\ref{table:learning}.
Note that in contrast to program verification, we do not have an oracle for determining if a reported error is a true error, or a false positive.
When learning a specification for a program, that specification can be checked by violating said specification and testing the program.
Because misconfigurations are dependent on the rest of the system, the available hardware, and the usage patterns, we cannot simulate the all conditions to determine if a reported error will cause system failure.
Indeed, as evidenced by Example~\ref{ex:fine}, some misconfigurations will only cause greater than expected performance degradation.
In this case, the very definition of a true error is necessarily imprecise.

\begin{table}[h]
\centering
\caption{Results of \app}
\label{table:learning}
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{|c|c|c|c|c|}
\hline
{\bf Class of Error } & {\bf \# Rules Learned} & {\bf \# Errors Detected} & {\bf Support} & {\bf Confidence}\\ 
\hline
\hline
Order        & 13  & 389  &  & \\ 
Missing      & 53  & 237  &  & \\ 
Type         & 92  & 1367 &  & \\ 
Fine-Grain   & 327 & 55   &  & \\ 
Coarse-Grain & 97  & 63   &  & \\ 
\hline 
\end{tabular}
\end{table}

The errors reported may have a varying impact on the system, ranging from failing to start, runtime crash, or performance degradation.
Since this is a probablistic system, it is also that some errors are false positives, a violation of the rule has no effect on the system.
It it therefore desirable to sort the reported errors in order of severity. 
To achieve this we use the post analysis metric described in Sec.~\ref{sec:ruleorder}.
To estimate the impact of this metric, we track the rank of known true positives with, and without, the augmented rule ordering in Table~\ref{table:order}.
We compare how close the updated rank is compared to the ideal rank, which was calculated by ennan and estimates the potential severity of an error related to those keywords.
Maybe we actually compare measure the distance from the ideal rank, so that a value of 0 is good.

\begin{table}[h]
\centering
\caption{Effect of Post Analysis Rule Ordering for known true positives}
\label{table:order}
\setlength{\tabcolsep}{1em}
\begin{tabular}{|c|c|c|c|}
\hline
{\bf Class of Error} & {\bf Reference} & {\bf Rank w/o} & {\bf Rank w/}\\
\hline
\hline
Order        & Ex.~\ref{ex:order}   &  & \\ 
Missing      & Ex.~\ref{ex:miss}    &  & \\ 
Type         & Ex.~\ref{ex:type}    &  & \\ 
Fine-Grain   & Ex.~\ref{ex:fine}    &  & \\ 
Coarse-Grain & Ex.~\ref{ex:coarse}  &  & \\ 
\hline
\end{tabular}
\end{table}

The checker stage of \app is the only one facing an end user that wants to verify a new configuration file.
Once a set of rules has been learned, it is not necessary to rerun the learner.
However, we have only used \app to build rules for MySQL, but any configuration language can be analyzed with \app given a training set.
In an industrial setting, the available training set may be much larger than ours as well, so is important that the learning process scales.
We see in Table~\ref{table:training} that \app scales roughly linearly.

\markk{TODO rewrite}
We also compare \app to prior work in configuration verification, ConfigC~\cite{santolucitoCAV}.
ConfigC scales exponentially because the learning algorithm assumes a completely correct training set, and learns every derivable relation.
With \app, we instead only process rules that meet the required support and confidence, reducing the cost of resolving to a consistent set of rules. 
The times reported in Table~\ref{table:training} were run on four cores of a Kaby Lake Intel Core i7-7500U CPU @ 2.70GHz on 16GB RAM and Fedora 25.

\begin{table}[h]
\centering
\caption{Time for training over various training set sizes}
\label{table:training}
\setlength{\tabcolsep}{1em}
\begin{tabular}{|c|c|c|}
\hline
{\bf \# of Files for Training} & {\bf ConfigC (sec)} & {\bf \app (sec)}\\ 
\hline
\hline
0    & 0.051    & 0.51  \\ \hline
50   & 1.815    & 1.638  \\ \hline
100  & 13.331   & 4.119  \\ \hline
150  & 95.547   & 4.231  \\ \hline
200  & 192.882  & 12.271  \\ \hline
256  & 766.904  & 15.627  \\ 
\hline
\end{tabular}
\end{table}

