
\section{Implementation and Evaluation}

We have implemented a \app prototype, and evaluated it based on
real-world configuration datasets.

\subsection{Prototype Implementation}

\ennan{put the implementation as one or more paragraphs.}

\subsection{Evaluation}

To evaluate our \app prototype, we need to have two datasets:
1)a training set and 2) a dataset for testing. 
For our training set, \trainingSet, we use a preexisting set of 256 
industrial MySQL configuration files collected in previous configuration 
analysis work~\cite{configdataset}.
The majority of configuration files in this training set are incorrect.
For our evaluation, we collected 1000 MySQL configuration files 
from Github, and filtered the poorly formatted files out for a final 
total of 973 files, called as {\em test set}.
In our evaluation we focus on MySQL for consistency, 
but \app can handle any configuration language (that can be parsed to 
the intermediate representation from Sec.~\ref{sec:trans}).

We report the number of rules learned from the training set and number of errors detected in the test set in Table~\ref{table:learning}.
Note that in contrast to program verification, we do not have an oracle for determining if a reported error is a true error, or a false positive, where a false positive means an error that has no impact on the system.
While we can run program to determine the effect a specification has on success of compiling or running the program, no such test exists for configuration files.
Because misconfigurations are dependent on the rest of the system, the available hardware, and the usage patterns, we cannot simulate the all conditions to determine if a reported error will cause system failure.
As evidenced by Example~\ref{ex:fine}, some misconfigurations will only cause greater than expected performance degradation, and only under particular traffic loads.
In this case, the definition of a true error is necessarily imprecise.

However, we are able to manually determine that some errors are true
positives by searching forums like StackOverflow for records that
particular configuration settings have cause problems on real-world
systems.


\begin{table}[h]
\centering
\caption{Results of \app}
\label{table:learning}
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{|c|c|c|c|c|}
\hline
{\bf Class of Error } & {\bf \# Rules Learned} & {\bf \# Errors Detected} & {\bf Support} & {\bf Confidence}\\ 
\hline
\hline
Order        & 13  & 62   & 6 \% & 94 \% \\ 
Missing      & 53  & 55   & 2 \%  & 71\% \\ 
Type         & 92  & 389  & -  & -  \\ 
Fine-Grain   & 213 & 324  & 24 \% & 91\%  \\ 
Coarse-Grain & 97  & 237  & 10 \% & 96\% \\ 
\hline 
\end{tabular}
\end{table}

Without ptypes - finegrained = 327 rules and 1367 detections

The errors reported may have a varying impact on the system, ranging from failing to start, runtime crash, or performance degradation.
Since this is a probablistic system, it is also that some errors are false positives, a violation of the rule has no effect on the system.
It it therefore desirable to sort the reported errors in order of severity. 
To achieve this we use the post analysis metric described in Sec.~\ref{sec:ruleorder}.
To estimate the impact of this metric, we track the rank of known true positives with, and without, the augmented rule ordering in Table~\ref{table:order}.
We compare how close the updated rank is compared to the ideal rank, which was calculated by ennan and estimates the potential severity of an error related to those keywords.
Maybe we actually compare measure the distance from the ideal rank, so that a value of 0 is good.

\input{secs/table2}

The checker stage of \app is the only one facing an end user that wants to verify a new configuration file.
Once a set of rules has been learned, it is not necessary to rerun the learner.
However, we have only used \app to build rules for MySQL, but any configuration language can be analyzed with \app given a training set.
In an industrial setting, the available training set may be much larger than ours as well, so is important that the learning process scales.
We see in Table~\ref{table:training} that \app scales roughly linearly.

\markk{TODO rewrite}
We also compare \app to prior work in configuration verification, ConfigC~\cite{santolucitoCAV}.
ConfigC scales exponentially because the learning algorithm assumes a completely correct training set, and learns every derivable relation.
With \app, we instead only process rules that meet the required support and confidence, reducing the cost of resolving to a consistent set of rules. 
The times reported in Table~\ref{table:training} were run on four cores of a Kaby Lake Intel Core i7-7500U CPU @ 2.70GHz on 16GB RAM and Fedora 25.

\begin{table}[h]
\centering
\caption{Time for training over various training set sizes}
\label{table:training}
\setlength{\tabcolsep}{1em}
\begin{tabular}{|c|c|c|}
\hline
{\bf \# of Files for Training} & {\bf ConfigC (sec)} & {\bf \app (sec)}\\ 
\hline
\hline
0    & 0.051    & 0.051  \\ \hline
50   & 1.815    & 1.638  \\ \hline
100  & 13.331   & 4.119  \\ \hline
150  & 95.547   & 10.232  \\ \hline
200  & 192.882  & 12.271  \\ \hline
256  & 766.904  & 15.627  \\ 
\hline
\end{tabular}
\end{table}

