
\section{Implementation and Evaluation}

We have implemented a tool, \app, and evaluated it based on real-world configuration files taken from Github.
\app is written in Haskell and is available open source at \textit{url redacted for anonymity}.
Thanks to the Haskell's powerful type system, the implementation can easily be extended with with new rule classes or applied to different configuration languages with minimal change to the rest of the code base.
A user only needs to provide the functions for the rule interface (a typeclass in Haskell) to 1) learn relations from a single file 2) merge two sets of rules and 3) check a file given some set of rules.

\subsection{Evaluation}

To evaluate our \app prototype, we require a separate training set and test set. 
For our training set, \trainingSet, we use a preexisting set of 256 
industrial MySQL configuration files collected in previous configuration 
analysis work~\cite{configdataset}.
This is an unlabeled training set, though most of the files have some errors.
For our test set, we collected 1000 MySQL configuration files 
from Github, and filtered the incorrectly formatted files out for a final 
total of 973 files.
In our evaluation we focus on MySQL for comparability of results, but \app can handle any configuration language (that can be parsed to the intermediate representation from Sec.~\ref{sec:trans}).

We report the number of rules learned from the training set and number of errors detected in the test set in Table~\ref{table:learning}.
We also provide the support and confidence thresholds, $t_s, t_c$, used in this evaluation.
These number can be adjusted by the user as a slider to control the level of assurance that their file is correct.
Since these settings depend on both the user preference and training set quality, we simply choose values for which \app reports reasonably sized output.

We record the histogram of errors across the test set in Fig.~\ref{fig:histo}.
This is intuitively an expected result from randomly sampling Github - most repositories will have few errors, with an increasingly small number of repositories having many errors.

\begin{figure}[h]
\includegraphics[width=\textwidth]{figs/histogram.png}
\caption{Histogram of errors - 14 errors were detected in 1 file}
\label{fig:histo}
\end{figure}

\begin{table}[h]
\centering
\caption{Results of \app}
\label{table:learning}
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{|c|c|c|c|c|}
\hline
{\bf Class of Error } & {\bf \# Rules Learned} & {\bf \# Errors Detected} & {\bf Support} & {\bf Confidence}\\ 
\hline
\hline
Order        & 13  & 62   & 6 \%  & 94 \% \\ 
Missing      & 53  & 55   & 2 \%  & 71\% \\ 
Type         & 92  & 389  & 12 \% & 70\%  \\ 
Fine-Grain   & 213 & 324  & 24 \% & 91\%  \\ 
Coarse-Grain & 97  & 237  & 10 \% & 96\% \\ 
\hline 
\end{tabular}
\end{table}

The errors reported may have varying impacts on the system, ranging from failing to start, runtime crash, or performance degradation.
However, since \app is a probabilistic system, it is also possible that some errors are false positives, a violation of the rule has no effect on the system.
Note that in contrast to program verification, we do not have an oracle for determining if a reported error is a true error or a false positive.
While we can run a program to determine the effect a specification has on the success of compiling/running the program, no such test exists for configuration files.
Because configurations are dependent on the rest of the system (\ie the available hardware, the network speed, and the usage patterns), we cannot simulate the all conditions to determine if a reported error will cause system failure.
As evidenced by Example~\ref{ex:fine}, some misconfigurations will only cause greater than expected performance degradation, and only under particular traffic loads.
In light of this, the definition of a true error is necessarily imprecise.

Although we cannot identify false positives, we can identify true positives by examining online forums, like StackOverflow, for record that
particular configuration settings have caused problems on real-world systems.
Furthermore, any error for which we can find evidence online is likely to be more problematic than errors that do not have an online record, 
  using the reasoning that this error has caused enough problems for people to seek help online.
In this case, we would like \app to can sort the errors by their importance or potential severity.
To achieve this sorting we use the rule graph analysis metric described in Sec.~\ref{sec:ruleorder}.

To estimate the impact of this metric, we track the rank of known true positives with, and without, the augmented rule ordering in Table~\ref{table-casestudy}.
For this table, we picked the known true positive rules, listed in the Errors column, and pick configuration files in the test set that have these errors.
We picked 3 files for each true positive by choosing the files the most total errors to most clearly observe the effects of our optimizations.
We test the following conditions; just rule graph analysis (RG) to sort the errors, just probabilistic types to filter the rules (PT), and both optimizations at the same time (RG $\land$ PT).
For each entry we list X/Y, where X is the rank of the known true positive, and Y is the total number of errors found on that file.
%Here we also compare the effect of probabilistic types - without probablitic types we learned 327 fin grainedrules and 1367 detections

\input{secs/table2}

We also evaluate the speed of \app.
Generally, once a set of rules has been learned, it is not necessary to rerun the learner.
However, we have only used \app to build rules for MySQL, but any configuration language can be analyzed with \app given a training set, which requires rerunning the learner.
Additionally, in an industrial setting, the available training set may be much larger than ours, so is important that the learning process scales.
We see in Table~\ref{table:training} that \app scales roughly linearly.

We compare \app to prior work in configuration verification, ConfigC~\cite{santolucitoCAV}.
ConfigC scales exponentially because the learning algorithm assumes a completely correct training set, and learns every derivable relation.
With \app, we instead only process rules that meet the required support and confidence, reducing the cost of resolving to a consistent set of rules. 
The times reported in Table~\ref{table:training} were run on four cores of a Kaby Lake Intel Core i7-7500U CPU @ 2.70GHz on 16GB RAM and Fedora 25.

\begin{table}[h]
\centering
\caption{Time for training over various training set sizes}
\label{table:training}
\setlength{\tabcolsep}{1em}
\begin{tabular}{|c|c|c|}
\hline
{\bf \# of Files for Training} & {\bf ConfigC (sec)} & {\bf \app (sec)}\\ 
\hline
\hline
0    & 0.051    & 0.051  \\ \hline
50   & 1.815    & 1.638  \\ \hline
100  & 13.331   & 4.119  \\ \hline
150  & 95.547   & 10.232  \\ \hline
200  & 192.882  & 12.271  \\ \hline
256  & 766.904  & 15.627  \\ 
\hline
\end{tabular}
\end{table}

