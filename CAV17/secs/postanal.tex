% Graph Analysis
% Rahul

\iffalse

# Structure and Core Points]
#
# @author rahuldhodapkar
#
#   Below is a structured summary from which I tried to write my section.
#   I have tried to include points of emphasis and a general argumentative
#   flow from which to work.
#
#   This should be used as a rubric to determine if we have hit all of
#   the critical beats of this section, or if there are core points
#   still missing.
#

[Problem Definition]

    - What is the intermediate representation from the learner?
        -> why can / should we interpret it as a graph?
        -> what benefits does this give over neural-networks?

    - What is the problem faced by the verification field?
        -> how can we be more intelligent about detecting false-positives?
        -> introduce the degree metric concept.

    - What are some problems outside the verification field that this can
      help answer?
        -> Introduce the idea of algorithmic complexity.
        -> We introduce a heuristic to measure the complexity of configuration
           files, drawn from information in the rule graph.

[The degree metric]

    - Definition of the degree metric.
        -> Sub-definition of edge slice sets.
        -> Sub-definition of edge set size => |*| operator

    - How will the degree metric be used to prune false-positive rules?
        -> specific definition of the weighting procedure.

[A heuristic for complexity of configuration files]

    - Definition of the complexity heuristic.

    - Example of the complexity heuristic on a graph (? space permitting)

    - Additional interpretations of the Rule Graph

\fi

\section{Post Analysis}

\app can output the set of learned rules from the training set as
described in $\S$\ref{sec-learn}. Recall that a rule is 
an implication relationship of the form $r = X \implies p(X,Y)$.
This data is sufficient for \app to perform its core task of
configuration file validation, but can be used for more than 
simple rule checking. By interpreting the rules as a graph (which
we call the {\bf Rule Graph}), we can 
use tools from graph theory to extract information
about the configuration space. We inspect properties of this 
Rule Graph to sort reported errors by those most likely to be valid.
To demonstrate the additional value of the Rule Graph, we
also use it to estimate the complexity of configuration settings.

Accessibility of the Rule Graph is a useful property of the association 
rule learning technique applied by \app.
In contrast, the model output from a neural network is not as easily 
amenable to analysis \cite{nnAnalysis1, nnAnalysis2}, making it harder
to build on the work of previously developed methods and tools.
The following section provides a precise definition of the Rule Graph
and the metrics derived for the purposes of ranking reported errors
and complexity analysis.

\subsection{Rule Ordering}
\label{sec:ruleorder}

We define the \textit{Rule Graph} as a directed hypergraph $H = (V,E)$,
   with vertices $V = \{ keywords \}$ and labeled, weighted edges $E = \{ (V_s, V_t, l, w) \}$.
The set of edges is constructed from the learned rules, using the source and target keyword sets as sources and targets respectively, the predicates as labels, and the confidence as weights:
%
\begin{align*}
\forall r \in & Learn(\trainingSet).\ \exists e\ \in E.\\
              & V_s = X_r \land V_t=Y_r \land l=p_r \land w = confidence(r)
\end{align*}

We will also denote $E_{V_1, V_2} \subset E$ as the \textit{slice set} 
of $E$ over $V_1, V_2$.
We can think of $E_{V_1, V_2}$ as being the subset of edges in $E$ 
such that each source set $V_s$ shares a vertex with $V_1$
and each target set $V_t$ shares a vertex with $V_2$.
Formally: 

\begin{align*}
    E_{V_1, V_2} = \{ \left( V_s, V_t, l, w \right) \in E \ \mid \ & \exists v_1 \in V_1 \land v_1 \in V_s \\
    & \land \exists v_2 \in V_2 \land v_2 \in V_t \}
\end{align*}


We denote a standalone vertex $v$ in our subscripts as notational 
convenience for the singleton set containing that vertex $v$.

The size of an edge set is the sum of all weights in that set, so:

    $$|E| = \sum_{(X, Y, l, w) \in E} w$$

The use of the support and confidence thresholds $t_s$ and $t_c$ in the learner ensure 
that all weights in the Rule Graph are positive.

We define a measure of degree $\mathcal{D}(v)$ for each vertex $v$ as the sum of 
in-degree and out-degree. Explicitly, for a vertex $x \in V$:

    $$\mathcal{D}(x) = \sum_{v \in V} |E_{x, v}| + \sum_{v \in V} |E_{v, x}|$$

We may now use this measure to rank our errors.
The more rules of high confidence are extracted for a keyword by the learner, 
the higher the $\mathcal{D}(v)$ of the corresponding vertex in the Rule Graph.
In our final analysis, we use this classification to
order the reported {\it errors} by estimated importance.

Keywords (specifically their corresponding vertices)
of low $\mathcal{D}(v)$ may be rarer configuration
parameters where rules learned are more likely to be governed by 
tehchnical necessity, rather than industry convention. As such, errors
reported involving low-degree keywords are more likely to be errors
of high significance and should be presented with high importance
to users of \app.

Specifically, for an error reported by \app on a rule $r$ involving
keywords $K$, we rank the errors by:

    $$RANK(r) = \frac{\sum_{k \in K} \mathcal{D}(k)}{|K|}$$

The results from ranking errors in this way are presented later in
the paper.

\subsection{Complexity Measure}

We may also use the Rule Graph to advance our general knowledge
of the configuration space, outside the strict confines of a
verification system. As an example, we present a heuristic
for configuration file complexity based on the topology of the
Rule Graph. This measure of complexity could be used by software
organizations to manage configuration files in much the same way
as Kolomogrov complexity~\cite{kolomogrov} is used to manage code - identifying
potentially brittle configurations for targeted refactoring.

\para {Definition}

For a configuration file with a set of keywords $K$
and a Rule Graph $H = (V, E)$, we define our complexity measure:

\begin{equation}
    \mathcal{C}(K, H) = \sum_{k \in K} \
        \begin{cases}
            1 * (1 - \frac{|E_{k, K}|}{|E_{k, V}|}) & \text{if}\ \ |E_{k, V}| > 0 \\
            1 & \text{otherwise}
        \end{cases}
\end{equation}

\para {Discussion}

The complexity measure, $\mathcal{C}$, can be thought of as an
extension of the na\"ive line-counting measure of complexity.
When a keyword in the configuration file is present in the Rule Graph,
we may consider the set $E_{k, K}$ to be all learned rules involving
keyword $k$ that
are {\it relevant} to the configuration file being examined. 
The set $E_{k, V}$ denotes {\it all} learned rules involving $k$.
Given these sets, we may think of $\frac{|E_{k, K}|}{|E_{k, V}|}$
as representing the amount that $k$ is constrained in the current
configuration file relative to how much it could be constrained
in the global configuration space. The more constrained a configuration
keyword in a particular configuration file, the {\it less} it should
contribute to the complexity (hence $1 * (1 - \frac{|E_{k, K}|}{|E_{k, V}|})$).
If a keyword is not constrained at all in the current configuration
file or is not present in the Rule Graph, we revert to the standard
counting metric of complexity. 

While an in-depth evaluation of the complexity metric presented here
is out of scope for this paper, we use this measure to
demonstrate the flexibility of the rule graph, and potential
for further applications.

