% Graph Analysis
% Rahul

\section{Post Analysis}

VeriConf can output the set of learned rules from the training set as described in $\S$\ref{sec-learn}.
Note that the ability to inspect the learned rule set is a special property of this technique.
The core task, building a model from unlabeled examples to classify files as either correct or incorrect, is also a classic problem for other machine learning techniques such as neural networks~\cite{nn1,nn2,nn3}.
When applying machine learning to verification of configuration files, this is not, however, an ideal approach.
The resulting model from a neural network is not easily amenable to analysis~\cite{nnAnalysis1,nnAnalysis2}.
The simple structure of rules with weights from an association rule learning-like approach, will instead allows us to do further inference on the resulting model.

\subsection{Rule Ordering}
\label{sec:ruleorder}

Define a directed hypergraph with labeled, weighted edges $H = (V,E)$ such that:

    $$V = \{ keywords \}$$

and 

    $$E = \{ (V_s, V_t, l, w) \}$$

Where $V_s$ and $V_t$ are sets of vertices. Intuitively, we may think of the
hyper-edge as being directed from $V_s$ to $V_t$. The label $l \in L$ indicates
the predicate $p$ extracted by the learner, where $L$ is the set of all
predicates learned. The weight $w$ is defined as:

    $$w = \frac{true}{total}$$

as measured by the learner for that relationship. For notational
convenience, later in this section
we shall define $w_{V_1, V_2, l}$ as the weight of the
hyperedge $(V_1, V_2, l, w)$, and $s_{v_1, v_2, l}$ as the sum of weights
over all hyperedges where $v_1 \in V_1$ and $v_2 \in V_2$.
We will refer to this hypergraph as the {\bf Rule Graph}
in further analysis.

Note that due to support / confidence assertions in the learner, we
may be assured that all weights in the graph are positive.

\para {Definition}
We define a measure of degree for each node as the sum of in-degree
and out-degree defined in the usual way. Explicitly, for a vertex $x \in V$:

    $$DEGREE(x) = OUT\_DEGREE(x) + IN\_DEGREE(x)$$

    $$DEGREE(x) = \sum_{l \in L} \sum_{v \in V} s_{x, v, l} + \sum_{l \in L} \sum_{v \in V} s_{v, x, l}$$

\iffalse

Note that we will be summing in-degree and out-degree over all
relationships $l$ to determine the total importance of
a keyword $k$ in the configuration space. We are restricted
by the total set of learning modules from which to derive
relationships $l$. If a keyword were highly constrained by some
relationship to other keywords in a way that might not be
measurable by current techniques, the learner (and
consequently the rule graph) will show these keywords as being
unrelated.

\fi

\para {Semantics}

The more rules of high confidence are extracted for a keyword by the learner, 
the higher the $DEGREE$ of the corresponding vertex in the Rule Graph.
We may use this measurement to examine which keywords in the configuration
space examined by the learner are most important - which keyword configurations
are the most constrained / most constraining.

Note that since the Rule Graph, as it is formulated in this paper, weights edges
by a {\it ratio} of predicate hits to total keyword observations, the weight
of an edge is {\bf dependent only on the confidence of the corresponding rule
and independent of the support}. We address this issue by pruning nodes of
inadequate support in the learner, but an alternative formulation of the rule
graph (in particular an alternative weighting of edges)
could provide additional
information for graph analysis techniques to take support into account
beyond rule confidence.

\rahul{Not sure how important this last point about alternative Rule Graph
formulation is - we certainly want to make sure the reader understands that
this is an area that can be easily expanded on, but don't want to distract
from the fundamental semantics here.}

It is also worth noting that how good of a heuristic the $DEGREE$ metric is for
\"importance of a configuration parameter\" is dependent on the coverage
of the learnable predicate set encompassed by $L$. The more this predicate
set covers the true configuration parameter relationships in the system,
the better the heuristic.

\iffalse

Intuitively, keywords that are seen together more often with defined relationships are more likely to generate rules within the learner.
 (it would be nice to show this with a quick derivation)

for a rule to be considered *true*, two conditions must hold:

1) adequate support -we have seen the keywords together x number of times

2) adequate confidence - how many times out of the x number of times was the relationship true?

\fi

\iffalse
- Open Questions
> What is the difference between {\it important} rules and
  {\it rules we are more sure about}?

> Why doesn't everything just sum to zero?

\fi
\subsection{Complexity Measure}

A common goal for software projects is managing complexity. As
constraints accumulate over the lifecycle of a project additional
configuration file complexity often grows, with the danger of
making code brittle and unwieldy. For code, Kolomogrov complexity
can be a good indicator of whether a solution is bloated and 
needs to be refactored.

The intermediate representation produced VeriConf's learner stage
may be used to analyze the complexity of a configuration file.

\para {Definition}

We define a measurement of complexity based on the calculated correlation.
For a configuration file with a set of keywords $K$ for a system
with a Rule Hypergraph $H = (V, E)$, we first define $E_{source}(V_s, V_t)$ as
the set of all hyperedges where a vertex $v_1 \in V_s$ is in the source set and
a vertex $v_2 \in V_t$ is in the target. Finally we define the size
of an edge set $|E|$ to be the sum over all weights in the edge set.

and may now define our complexity measure:

\begin{equation}
    COMPLEXITY(K, H) = \sum_{k \in K} \
        \begin{cases}
            1 * (1 - \frac{|E(k, K)|}{|E(k, V)|}) & \text{if}\ \ |E(k, V)| > 0 \\
            1 & \text{otherwise}
        \end{cases}
\end{equation}

\para {Semantics}

The presented metric is a heuristic that intends to approximate the question: 
how much redundancy exists in a configuration file, when considering
probabilistic constraints imposed by the problem domain? It does this by
expanding on a naive metric of complexity - the number of configuration 
parameters. Consider the following set of configuration files, drawing from
an example presented earlier in this paper.

\begin{figure}[!htb]
    \centering
    \begin{minipage}{.25\textwidth}
    \begin{lstlisting} [label={lst:1.cnf},language=C,caption={1.cnf}]
[server]
foo = ON
[client]
bar = 1
    \end{lstlisting}
    \end{minipage}%
    \hspace{1cm}
    \begin{minipage}{0.25\textwidth}
    \begin{lstlisting} [label={lst:2.cnf},language=C,caption={2.cnf}]
[server]
foo = ON
[client]
bar = ON
    \end{lstlisting}
    \end{minipage}
    \hspace{1cm}
    \begin{minipage}{0.25\textwidth}
    \begin{lstlisting} [label={lst:2.cnf},language=C,caption={3.cnf}]
[server]
pot = ON
[client]
cat = ON
    \end{lstlisting}
    \end{minipage}
    \caption{A learning set of configuration files}
    \label{fig:complexityset}
\end{figure}

From our learner, we determined that the parameters $foo$ and $bar$ are
highly correlated with each other, by an equality relation. In fact, this
is the only relation learned for $foo$ and $bar$ within the configuration
space. $pot$ and $cat$ on the other hand, were determined to be unrelated
by the learner. If using the naive counting measure of complexity, all 
three of these configuration files would have the same measure of $2$.

However, we know that $foo$ and $bar$ are not independent settings.
The fact that they are both set to a truthy value communicates less
information than the settings for the file with $pot$ and $cat$.

Our defined complexity measure would evaluate to $1$ on the
configuration file with $foo$ and $bar$, while evaluating to $2$ on
the configuration file with $pot$ and $cat$, more accurately representing
the amount of information actually conveyed by these settings. Note,
however that our metric does not actually consider the {\it values}
of the configuration settings when determining complexity. In the
future, it may be possible in the future to have interpreted predicates,
such that these heuristics may even more closely reflect the informational
content of a constrained configuration file.

We hope that further researchers will expand on the foundation we lay
here to extract more useful information from the intermediate
representation built by the VeriConf learner.

\iffalse

it may be possible in the future to have interpreted predicates,
such that these heuristics may more accurately reflect the informational
content of a constrained configuration file.

Use the listing in figure 2 to put together an example of why the
naive implementation of complexity doesn't give as much of an idea
of what is going on as our heuristic.

Additional refinement of a complexity metric could take into account
when learned rules are violated (indicating potentially brittle
configuration state)

\fi





