% Graph Analysis
% Rahul

\section{Post Analysis}

VeriConf can output the set of learned rules from the training set as described in $\S$\ref{sec-learn}.
Note that the ability to inspect the learned rule set is a special property of this technique.
The core task, building a model from unlabeled examples to classify files as either correct or incorrect, is also a classic problem for other machine learning techniques such as neural networks~\cite{nn1,nn2,nn3}.
When applying machine learning to verification of configuration files, this is not, however, an ideal approach.
The resulting model from a neural network is easily not amenable to analysis~\cite{nnAnalysis1,nnAnalysis2}.
The simple structure of rules with weights from an association rule learning-like approach, will instead allows us to do further inference on the resulting model.

\subsection{Rule Ordering}
\label{sec:ruleorder}

Define a graph with labeled, weighted edges $G = (V,E)$ such that:

    $$V = \{ keywords \}$$

and 

    $$E = \{ (v_1, v_2, l, w) \}$$

Where the label $l \in L$ indicates the predicate $p$ extracted by the learner,
$L$ is the set of all predicates learned,
and the weight $w$ is defined as 

    $$w = \frac{true - false}{total}$$

for that relationship. For notational convenience later in this section
we will refer to $w_{v_1, v_2, l}$ as referring to the weight of the
edge $(v_1, v_2, l, w)$. We will refer to this graph as the {\bf Rule Graph}
in further analysis.

Note that due to support / confidence assertions in the learner, we
may be assured that all weights in the graph are positive.

\para {Definition}
We define a measure of degree for each node as the sum of in-degree
and out-degree defined in the usual way. Explicitly, for a vertex $x \in V$:

    $$DEGREE(x) = OUT\_DEGREE(x) + IN\_DEGREE(x)$$

    $$DEGREE(x) = \sum_{l \in L} \sum_{v \in V} w_{x, v, l} + \sum_{l \in L} \sum_{v \in V} w_{v, x, l}$$

The more rules of high confidence extracted for a keyword by the learner, 
the higher the $DEGREE$ of the corresponding vertex in the Rule Graph.
We may use this measurement to examine which keywords in the configuration
space examined by the learner are most important - which keyword configurations
are the most constrained / most constraining.

Note that we will be summing in-degree and out-degree over all
relationships $l$ to determine the total importance of
a keyword $k$ in the configuration space. We are restricted
by the total set of learning modules from which to derive
relationships $l$. If a keyword were highly constrained by some
relationship to other keywords in a way that might not be
measurable by current techniques, the learner (and therefore
consequently the graph) will show these keywords as being
unrelated.

\para {Semantics}
Intuitively, keywords that are seen together more often with defined relationships are more likely to generate rules within the learner.
 (it would be nice to show this with a quick derivation)

for a rule to be considered *true*, two conditions must hold:

1) adequate support -we have seen the keywords together x number of times

2) adequate confidence - how many times out of the x number of times was the relationship true?

\iffalse
- Open Questions
> What is the difference between {\it important} rules and
  {\it rules we are more sure about}?

> Why doesn't everything just sum to zero?

\fi
\subsection{Complexity Measure}

INCLUDE SHORT BACKGROUND ON CODE COMPLEXITY. We define a measurement
of complexity based on the calculated correlation. The presented
metric is a heuristic that intends to approximate the quetion, 
 how much redundancy exists in a configuration file, when considering
probabilistic constraints imposed by the problem domain?

it may be possible in the future to have interpreted predicates,
such that these heuristics may more accurately reflect the informational
content of a constrained configuration file.




