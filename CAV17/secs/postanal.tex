% Graph Analysis
% Rahul

\section{Post Analysis}

\iffalse

# Structure and Core Points]
#
# @author rahuldhodapkar
#
#   Below is a structured summary from which I tried to write my section.
#   I have tried to include points of emphasis and a general argumentative
#   flow from which to work.
#
#   This should be used as a rubric to determine if we have hit all of
#   the critical beats of this section, or if there are core points
#   still missing.
#

[Problem Definition]

    - What is the intermediate representation?
        -> why can / should we interpret it as a graph?
        -> what benefits does this give over neural-networks?

    - What is the problem faced by the verification field?
        -> how can we be more intelligent about detecting false-positives?
        -> introduce the degree metric concept.

    - What are some problems outside the verification field that this can
      help answer?
        -> Introduce the idea of algorithmic complexity.
        -> We introduce a heuristic to measure the complexity of configuration
           files, drawn from information in the rule graph.

[The degree metric]

    - Definition of the degree metric.
        -> Sub-definition of edge slice sets.
        -> Sub-definition of edge set size => |*| operator

    - How will the degree metric be used to prune false-positive rules?
        -> specific definition of the weighting procedure.

[A heuristic for complexity of configuration files]

    - Definition of the complexity heuristic.

    - Example of the complexity heuristic on a graph (? space permitting)

    - Additional interpretations of the Rule Graph

\fi

RULE GRAPH

VeriConf can output the set of learned rules from the training set as described in $\S$\ref{sec-learn}.
Note that the ability to inspect the learned rule set is a special property of this technique.
The core task, building a model from unlabeled examples to classify files as either correct or incorrect, is also a classic problem for other machine learning techniques such as neural networks~\cite{nn1,nn2,nn3}.
When applying machine learning to verification of configuration files, this is not, however, an ideal approach.
The resulting model from a neural network is not easily amenable to analysis~\cite{nnAnalysis1,nnAnalysis2}.
The simple structure of rules with weights from an association rule learning-like approach, will instead allows us to do further inference on the resulting model.

\subsection{Rule Ordering}
\label{sec:ruleorder}

Define a directed hypergraph with $H = (V,E)$ labeled, weighted edges such that:

    $$V = \{ keywords \}$$

and 

    $$E = \{ (V_s, V_t, l, w) \}$$

Where $V_s$ and $V_t$ are sets of vertices. Intuitively, we may think of the
hyper-edge as being directed from $V_s$ to $V_t$. The label $l \in L$ indicates
the predicate $p$ extracted by the learner, where $L$ is the set of all
predicates learned. The weight $w$ is defined as:

    $$w = \frac{true}{total}$$

as measured by the learner for that relationship. We will also denote
$E_{V_1, V_2}$ as
the set of all hyperedges where a vertex $v_1 \in V_s$ is in the 
source set and a vertex $v_2 \in V_t$ is in the target. Sometimes we
will write a single vertex $v$ in our subscript, as notational convenience
for the singleton set containing $v$.

The size of an edge set is the sum of all weights in that set, so:

    $$|E_{v_1, v_2}| = \sum_{(X, Y, l, w) \in E_{v_1, v_2}} w$$

We will refer to this hypergraph as the {\bf Rule Graph}
in further analysis.

We note that support and confidence assertions in the learner ensure
that all weights are positive.

\para {Definition}
We define a measure of degree for each node as the sum of in-degree
and out-degree defined in the usual way. Explicitly, for a vertex $x \in V$:

    $$DEGREE(x) = OUT\_DEGREE(x) + IN\_DEGREE(x)$$

    $$DEGREE(x) = \sum_{v \in V} |E_{x, v}| + \sum_{v \in V} |E_{v, x}|$$

Talk more about hyperedges or some shit
\iffalse

Note that we will be summing in-degree and out-degree over all
relationships $l$ to determine the total importance of
a keyword $k$ in the configuration space. We are restricted
by the total set of learning modules from which to derive
relationships $l$. If a keyword were highly constrained by some
relationship to other keywords in a way that might not be
measurable by current techniques, the learner (and
consequently the rule graph) will show these keywords as being
unrelated.

\fi

\para {Semantics}

The more rules of high confidence are extracted for a keyword by the learner, 
the higher the $DEGREE$ of the corresponding vertex in the Rule Graph.
We may use this measurement to examine which keywords in the configuration
space examined by the learner are most important - which keyword configurations
are the most constrained / most constraining.

Note that since the Rule Graph, as it is formulated in this paper, weights edges
by a {\it ratio} of predicate hits to total keyword observations, the weight
of an edge is {\bf dependent only on the confidence of the corresponding rule
and independent of the support}. We address this issue by pruning nodes of
inadequate support in the learner, but an alternative formulation of the rule
graph (in particular an alternative weighting of edges)
could provide additional
information for graph analysis techniques to take support into account
beyond rule confidence.

\rahul{Not sure how important this last point about alternative Rule Graph
formulation is - we certainly want to make sure the reader understands that
this is an area that can be easily expanded on, but don't want to distract
from the fundamental semantics here.}

It is also worth noting that how good of a heuristic the $DEGREE$ metric is for
importance of a configuration parameter is dependent on the coverage
of the learnable predicate set encompassed by $L$. The more this predicate
set covers the true configuration parameter relationships in the system,
the better the heuristic.

\iffalse

Intuitively, keywords that are seen together more often with defined relationships are more likely to generate rules within the learner.
 (it would be nice to show this with a quick derivation)

for a rule to be considered *true*, two conditions must hold:

1) adequate support -we have seen the keywords together x number of times

2) adequate confidence - how many times out of the x number of times was the relationship true?

\fi

\iffalse
- Open Questions
> What is the difference between {\it important} rules and
  {\it rules we are more sure about}?

> Why doesn't everything just sum to zero?

\fi
\subsection{Complexity Measure}

A common goal for software projects is managing complexity. As
constraints accumulate over the lifecycle of a project additional
configuration file complexity often grows, with the danger of
making code brittle and unwieldy. For code, Kolomogrov complexity
can be a good indicator of whether a solution is bloated and 
needs to be refactored.

The intermediate representation produced VeriConf's learner stage
may be used to analyze the complexity of a configuration file.

\para {Definition}

We define a measurement of complexity based on the calculated correlation.
For a configuration file with a set of keywords $K$ for a system
with a Rule Hypergraph $H = (V, E)$.

and may now define our complexity measure:

\begin{equation}
    COMPLEXITY(K, H) = \sum_{k \in K} \
        \begin{cases}
            1 * (1 - \frac{|E_{k, K}|}{|E_{k, V}|}) & \text{if}\ \ |E_{k, V}| > 0 \\
            1 & \text{otherwise}
        \end{cases}
\end{equation}

Since all weights are positive, $|E_{k, V}| > 0$ if there is at least one 
edge in $E_{k, V}$. Examining $\frac{|E_{k, K}|}{|E_{k, V}|}$, we may think of this
term as the ratio of {\it possible} constraints on a keyword $k$ by all keywords
in the configuration space ($V$) to the {\it actual} constraints imposed in the
examined configuration file ($K$).

Waste less time explaining the example (that should be maybe 1 or 2 paragraphs) and more time explaining the complexity measure.
\para {Semantics}

The presented metric is a heuristic that intends to approximate the question: 
how much redundancy exists in a configuration file, when considering
probabilistic constraints imposed by the problem domain? It does this by
expanding on a naive metric of complexity - the number of configuration 
parameters. Consider the following set of configuration files, drawing from
an example presented earlier in this paper.

\begin{figure}[!htb]
    \centering
    \begin{minipage}{.25\textwidth}
    \begin{lstlisting} [label={lst:1.cnf},language=C,caption={1.cnf}]
[server]
foo = ON
[client]
bar = 1
    \end{lstlisting}
    \end{minipage}%
    \hspace{1cm}
    \begin{minipage}{0.25\textwidth}
    \begin{lstlisting} [label={lst:2.cnf},language=C,caption={2.cnf}]
[server]
foo = ON
[client]
bar = ON
    \end{lstlisting}
    \end{minipage}
    \hspace{1cm}
    \begin{minipage}{0.25\textwidth}
    \begin{lstlisting} [label={lst:2.cnf},language=C,caption={3.cnf}]
[server]
pot = ON
[client]
cat = ON
    \end{lstlisting}
    \end{minipage}
    \caption{A learning set of configuration files}
    \label{fig:complexityset}
\end{figure}

From our learner, we determined that the parameters $foo$ and $bar$ are
highly correlated with each other, by an equality relation. In fact, this
is the only relation learned for $foo$ and $bar$ within the configuration
space. $pot$ and $cat$ on the other hand, were determined to be unrelated
by the learner. If using the naive counting measure of complexity, all 
three of these configuration files would have the same measure of $2$.

However, we know that $foo$ and $bar$ are not independent settings.
The fact that they are both set to a truthy value communicates less
information than the settings for the file with $pot$ and $cat$.

Our defined complexity measure would evaluate to $1$ on the
configuration file with $foo$ and $bar$, while evaluating to $2$ on
the configuration file with $pot$ and $cat$, more accurately representing
the amount of information actually conveyed by these settings. Note,
however that our metric does not actually consider the {\it values}
of the configuration settings when determining complexity. In the
future, it may be possible in the future to have interpreted predicates,
such that these heuristics may even more closely reflect the informational
content of a constrained configuration file.

We hope that further researchers will expand on the foundation we lay
here to extract more useful information from the intermediate
representation built by the VeriConf learner.

\iffalse

it may be possible in the future to have interpreted predicates,
such that these heuristics may more accurately reflect the informational
content of a constrained configuration file.

Use the listing in figure 2 to put together an example of why the
naive implementation of complexity doesn't give as much of an idea
of what is going on as our heuristic.

Additional refinement of a complexity metric could take into account
when learned rules are violated (indicating potentially brittle
configuration state)

\fi





