% Graph Analysis
% Rahul

\iffalse

# Structure and Core Points]
#
# @author rahuldhodapkar
#
#   Below is a structured summary from which I tried to write my section.
#   I have tried to include points of emphasis and a general argumentative
#   flow from which to work.
#
#   This should be used as a rubric to determine if we have hit all of
#   the critical beats of this section, or if there are core points
#   still missing.
#

[Problem Definition]

    - What is the intermediate representation from the learner?
        -> why can / should we interpret it as a graph?
        -> what benefits does this give over neural-networks?

    - What is the problem faced by the verification field?
        -> how can we be more intelligent about detecting false-positives?
        -> introduce the degree metric concept.

    - What are some problems outside the verification field that this can
      help answer?
        -> Introduce the idea of algorithmic complexity.
        -> We introduce a heuristic to measure the complexity of configuration
           files, drawn from information in the rule graph.

[The degree metric]

    - Definition of the degree metric.
        -> Sub-definition of edge slice sets.
        -> Sub-definition of edge set size => |*| operator

    - How will the degree metric be used to prune false-positive rules?
        -> specific definition of the weighting procedure.

[A heuristic for complexity of configuration files]

    - Definition of the complexity heuristic.

    - Example of the complexity heuristic on a graph (? space permitting)

    - Additional interpretations of the Rule Graph

\fi

\section{Post Analysis}

\app can output the set of learned rules from the training set as
described in $\S$\ref{sec-learn}, recalling that a rule is 
an implication relationship of the form $r = X \implies p(X,Y)$.
This data is sufficient for \app to perform its core task of
configuration file validation, but can be used for more than 
simple rule checking. By re-interpreting
rules as being the edges of a {\bf Rule Graph}, we use tools from network
theory to extract information
about the configuration space. We inspect properties of the Rule Graph 
to triage reported errors by those most likely to be valid,
and even to estimate the complexity of configuration settings.

Accessibility of the Rule Graph is a hallmark of the association rule
learning technique applied by \app \cite{someOtherGraphStuff}.
In contrast, the model output from a neural network is not as easily 
amenable to analysis \cite{nnAnalysis1, nnAnalysis2}, making it harder
to build on the work of previously developed methods and tools.

The following section provides a precise definition of the Rule Graph
and the metrics derived for the purposes of ranking reported errors
and complexity analysis.

\subsection{Rule Ordering}
\label{sec:ruleorder}

We define the \textit{Rule Graph} as a directed hypergraph $H = (V,E)$,
   with vertices $V = \{ keywords \}$ and labeled, weighted edges $E = \{ (V_s, V_t, l, w) \}$.
The set of edges is constructed from the learned rules, using the keyword sets as source and targets, the predicates as labels, and the confidence as weights:
%
\begin{align*}
\forall r \in & Learn(\trainingSet).\ \exists e\ \in E.\\
              & V_s = X_r \land V_t=Y_r \land l=p_r \land w = confidence(r)
\end{align*}

We will also denote $E_{V_1, V_2} \subset E$ as the \textit{slice set} of $E$ over $V_1, V_2$: 

    $$E_{V_1, V_2} = \{ \left( V_s, V_t, l, w \right) \in E \ \mid \ \exists v_1 \in V_1, V_s \land \exists v_2 \in V_2, V_t \}$$

Sometimes we will write a single vertex $v$ in our subscript, as
notational convenience for the singleton set containing $v$.

The size of an edge set is the sum of all weights in that set, so:

    $$|E_{v_1, v_2}| = \sum_{(X, Y, l, w) \in E_{v_1, v_2}} w$$

The use of the support and confidence thresholds $t_s$ and $t_c$ in the learner ensure 
that all weights in the Rule Graph are positive.

\para {Definition}
We define a measure of degree $\mathcal{D}(v)$ for each vertex $v$ as the sum of in-degree
and out-degree defined in the usual way. Explicitly, for a vertex $x \in V$:

    $$\mathcal{D}(x) = \sum_{v \in V} |E_{x, v}| + \sum_{v \in V} |E_{v, x}|$$

\para {Discussion}

The more rules of high confidence are extracted for a keyword by the learner, 
the higher the $DEGREE$ of the corresponding vertex in the Rule Graph.
By classifying keywords in this way, we may extract a simple way to
order the {\it errors} reported by \app by estimated importance.

We hypothesize that keywords (specifically their corresponding vertices)
of low $DEGREE$ will be rarer configuration
parameters where rules learned are more likely to be governed by 
tehchnical necessity, rather than industry convention. As such, errors
reported involving low-degree keywords are more likely to be errors
of high significance and should be presented with high importance
to users of \app.

Specifically, for an error reported by \app on a rule $r$ involving
keywords $K$, we may rank the errors simply by:

    $$RANK(r) = avg( DEGREE(K) )$$

The results from ranking errors in this way are presented later in
the paper.

\subsection{Complexity Measure}

We may also use the Rule Graph to advance our general knowledge
of the configuration space, outside the strict confines of a
verification system. As an example, we briefly present a heuristic
for configuration file complexity based on the topology of the
Rule Graph. This measure of complexity could be used by software
organizations to manage configuration files in much the same way
as Kolomogrov complexity~\cite{kolomogrov} is used to manage code - identifying
potentially brittle configurations for targeted refactoring.

\para {Definition}

For a configuration file with a set of keywords $K$ for a system
and a Rule Graph $H = (V, E)$, we define our complexity measure:

\begin{equation}
    \mathcal{C}(K, H) = \sum_{k \in K} \
        \begin{cases}
            1 * (1 - \frac{|E_{k, K}|}{|E_{k, V}|}) & \text{if}\ \ |E_{k, V}| > 0 \\
            1 & \text{otherwise}
        \end{cases}
\end{equation}

\para {Discussion}

The complexity measure, $\mathcal{C}$, can be thought of as an
extension of the na\"ive line-counting measure of complexity.
When a keyword in the configuration file is present in the Rule Graph,
we may consider the set $E_{k, K}$ to be all learned rules involving
keyword $k$ that
are {\it relevant} to the configuration file being examined. the set
$E_{k, V}$ denotes {\it all} learned rules involving $k$.
Given these sets, we may think of $\frac{|E_{k, K}|}{|E_{k, V}|}$
as representing the amount that $k$ is constrained in the current
configuration file relative to how much it could be constrained
in the global configuration space. The more constrained a configuration
keyword in a particular configuration file, the {\it less} it should
contribute to the complexity (hence $1 * (1 - \frac{|E_{k, K}|}{|E_{k, V}|})$).

If a keyword is not constrained at all in the current configuration
file or is not present in the Rule Graph, we revert to the standard
counting metric of complexity.

