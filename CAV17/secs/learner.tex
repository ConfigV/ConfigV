\section{Learner}
\label{sec-learn}

The goal of the learner is to derive rules from the intermediate representation of the training set generated by the translator.
We describe an interface to define the different classes of rules that should be learned.
Each instance of the interface corresponds to a different class of of configuration errors, such as missing entry errors, ordering errors, and integer correlation errors. 
These errors can cause total system failures, but can also be more insidious, for example slowing down the system only when the server load increases beyond a certain threshold.

\rahul{and we may even be able to say that these insidiuous issues are much
harder to pin down by the standard delta-debugging technique of starting a
system multiple times with different configuration settings}

A rule is an implication relationship, $X \implies p(X,Y)$, between two possibly empty sets of keywords $X,Y \subset \keys$, where $\keys$ is the set of unique keys from the training set and the predicate $p$ is the one of the classes of configuration errors (order, missing, etc).
Implicitly we interpret this to means that if the keywords $X\cup Y$ appear in a configuration file, we expect the predicate $p$ to hold.
The task of the learning algorithm is to transform a training set to a set of rules, weighted with \textit{support} and \textit{confidence}.
These two key metrics are taken from \textit{association rule learning}~\cite{agrawal1993mining}, a technique that can be summarized as inductive machine learning.
In the configuration verification domain, standard association rule learning is best suited to learn integer correlation and missing keyword rules.
In fact, a more specialized technique for learning ordering rules would be sequence mining~\cite{}, and again another approach may be a better fit for rules over a single keyword, as is needed for probabilistic types.
Since the full algorithm details are out-of-scope of this paper, here we only describe in detail the metrics which are most relevant to verification.

%TODO math def of support and confidence
Each power set of keywords, $\{X,Y\}$, is assigned a support and confidence measure during the learning process.
Support is the number of times the set of keywords in the proposed rule have been seen the in the training set.
Confidence is the number of times the rule predicate has held true over the given keywords.
In the learning process, these are given threshold, below which a rule will be reject for lack of evidence.

\para{Order}
Ordering errors take the form $X \implies order(X,Y)$, where $|X|,|Y|=1$ and the keyword $X$ must come before the keyword $Y$ in any configuration file.

\para{Missing}
$X \implies missing(X,Y)$, where $|X|,|Y|=1$ and the keyword $X$ must appear in the same file as the keyword $Y$ in any configuration file.

\para{Type}
The type rule is a set of rules over multiple predicates, which take the form $X \implies isType\ast(X)$, where $|X|=1, |Y|=0$ and $\ast$ matches all the basic types (string, int, etc).
In \app, probabilistic typing is implemented as an instance of the learning interface.
This module will specify the counting and resolution type judgments from Sec.~\ref{sec:ptypes}.
It 

\para{Integer Correlation}
\app supports two types of integer correlation rules, coarse-grained and fine-grained.
Coarse-grained rules follow $X \implies compare(X,Y)$, where $|X|,|Y|=1$ and $compare \in \{<,=,>\}$, such that $X$ must hold $compare$ to $Y$.
Fine-grained rules follow $X \implies compare(X,Y)$, where $|X|=2,|Y|=1$ and $compare \in \{<,=,>\}$, such that for $k_1,\ k_2 \in X,\ k1*k2$ must hold $compare$ to $Y$.
These rules also implement a typing judgment over the keywords probabilistic type.
To avoid learning too many false positives, we restrict this rule to either $size*int=size$, $int*size=size$, or $int*int = int$.
Without probabilistic typing, we would also learn, for example, $int*int=size$.
%TODO include exactly how many false positive we prune with types over our training set? Here or in eval?

%TODO Aaron - add to learinign module section describe implemetnation?

\app is primarily implemented in Haskell.
The source code for our implementation is available at {\em (URL omitted for blind review)}.
 
The learning and checking modules are developed to allow
for customized extensibility. In order to verify a configuration file
against a new type of error, a user only needs to provide a new type
that is an instance of the \textit{Attribute} typeclass (\ie, define some
functions over that type). 
In particular, for each type of error the users
wish to detect, they must implement three functions;
\lstinline{learn}, \lstinline{merge}, and \lstinline{check}.


\subsection{Checker}
\label{sec-checker}

With the learned rules generates by the learner module,
  \app checks whether any entry in a target configuration file violates the learned rules and constraints.
\app parses a verification target configuration file the same way employed in the translator for learning,
  obtaining a structured and typed representation.
Then, the checker applies the learner to build the set of relations observed in the file.
For any relation that violates a known rule, the checker will output the predicate and keyword sets associated with that rule, as well as the learned support and confidence values.
Since the tool is probabilistic, we provide the user with these values to determine if they rule must be satisfied in their application on a case-by-case basis.
For instance, the \texttt{key\_buffer} misconfiguration from Sec. \ref{ex:fine} will only be noticeable if the application experiences a heavy traffic load, so the user may choose to ignore this error if they are confident this will not be an issue.


\subsection{Implementation}


