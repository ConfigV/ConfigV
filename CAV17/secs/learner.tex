\section{Learner}
\label{sec-learn}

The goal of the learner is to derive rules from the intermediate representation of the training set generated by the translator.
We describe an interface to define the different classes of rules that should be learned.
Each instance of the interface corresponds to a different class of of configuration errors, such as missing entry errors, ordering errors, and integer correlation errors. 
These errors can cause total system failures, but can also be more insidious, for example slowing down the system only when the server load increases beyond a certain threshold.

\rahul{and we may even be able to say that these insidiuous issues are much
harder to pin down by the standard delta-debugging technique of starting a
system multiple times with different configuration settings}

A rule is an implication relationship, $X \implies p(X,Y)$, between two possibly empty sets of keywords $X,Y \subset \keys$, where $\keys$ is the set of unique keys from the training set and the predicate $p$ is the one of the classes of configuration errors (order, missing, etc).
Implicitly we interpret this to means that if the keywords $X\cup Y$ appear in a configuration file, we expect the predicate $p$ to hold.
The task of the learning algorithm is to transform a training set to a set of rules, weighted with \textit{support} and \textit{confidence}.
These two key metrics are taken from \textit{association rule learning}~\cite{agrawal1993mining}, a technique that can be summarized as inductive machine learning.
In the configuration verification domain, standard association rule learning is best suited to learn integer correlation and missing keyword rules.
In fact, a more specialized technique for learning ordering rules would be sequence mining~\cite{}, and again another approach may be a better fit for rules over a single keyword, as is needed for probabilistic types.
Since the full algorithm details are out-of-scope of this paper, here we only describe in detail the metrics which are most relevant to verification.

%TODO math def of support and confidence
Each set of keyword sets, $\{X,Y\}$, is assigned a support and confidence measure during the learning process.
Support is the number of times the set of keywords in the proposed rule have been seen the in the training set.
Confidence is the number of times the rule predicate has held true over the given keywords.
In the learning process, these are given threshold, below which a rule will be reject for lack of evidence.

\para{Order}
Ordering errors take the form $X \implies order(X,Y)$, where $|X|,|Y|=1$ and the keyword $X$ must come before the keyword $Y$ in any configuration file.

\para{Missing}
$X \implies missing(X,Y)$, where $|X|,|Y|=1$ and the keyword $X$ must appear in the same file as the keyword $Y$ in any configuration file.

\para{Type}
The type rule is a set of rules over multiple predicates, which take the form $X \implies isType\ast(X)$, where $|X|=1, |Y|=0$ and $\ast$ matches all the basic types (string, int, etc).
In \app, probabilistic typing is implemented as an instance of the learning interface.
This module will specify the counting and resolution type judgments from Sec.~\ref{sec:ptypes}.
It 

\para{Integer Correlation}
\app supports two types of integer correlation rules, coarse-grained and fine-grained.
Coarse-grained rules follow $X \implies compare(X,Y)$, where $|X|,|Y|=1$ and $compare \in \{<,=,>\}$, such that $X$ must hold $compare$ to $Y$.
Fine-grained rules follow $X \implies compare(X,Y)$, where $|X|=2,|Y|=1$ and $compare \in \{<,=,>\}$, such that for $k_1,\ k_2 \in X,\ k1*k2$ must hold $compare$ to $Y$.
These rules also implement a typing judgment over the keywords probabilistic type.
To avoid learning too many false positives, we restrict this rule to either $size*int=size$, $int*size=size$, or $int*int = int$.
Without probabilistic typing, we would also learn, for example, $int*int=size$.
%TODO include exactly how many false positive we prune with types over our training set? Here or in eval?

\app is primarily implemented in Haskell.
The source code for our implementation is available at {\em (URL omitted for blind review)}.

We have developed \app to be easily extendable and customizable. Each of the rules are implemented as an instance of the \textit{Learnable} typeclass:

\begin{lstlisting}[language=Haskell, xleftmargin=.01\textwidth]
class (Eq a, Show a, Ord a, Countable b) => Learnable a b where
  buildRelations :: IRConfigFile -> RuleDataMap a b
  merge :: Countable b => [RuleDataMap a b] -> RuleDataMap a b
  check :: a -> b -> b -> Maybe b
  toError :: FilePath -> (a, b) -> Error
\end{lstlisting} 

Each of the rules that check for a particular type of error are implemented in three functions, \textit{buildRelations} for constructing a map of possible observations to a count of how often they are observed, a \textit{merge} function for taking multiple sets of built relations from different files and combining them into one set of relations (weighted accordingly), and finally, a \textit{check} that takes an individual relation mapping from our built set of relations, the observation in the file we are verifying, and whether there is a violation of that particular observation of the relation. (There is a fourth \textit{toError} function that serves as a helper function to pretty-print the output of \app.) Hence, if we can define how to build the relations to verify against a new type of error, a system for combining these relations with weights across multiple files that are seen, along with a mechanism for checking whether each individual relation was violated or not, we can extend \app to check for new types of errors. Furthermore, the approach of defining a method to build relations from a single file as well as a method to combine these relations facilitates the parallelization of our implementation, since each of the \textit{buildRelations} calls can work parlellely on each file before the results are combined with the \textit{merge} step.

Each of the outputs of the \textit{buildRelations} is in the form \textit{RuleDataMap a b}, where \textit{a} is a group of entities between which we want to define a relation (for example, a pair of keywords for Missing, Order, and Integer Correlation) and \textit{Countable b} is a tally of likelihoods of each possible outcome of the relation that can be weighted and combined over multiple files (for example, a count of files that exhibit the relation versus not for Missing or a count of files that exhibit less than, equals, or greater than between a pair of keywords for Ordering or a set of probabilities for Type rules).

Once the program has finished merging together a set of built relations over every example file in the learning set, we have implemented a \textit{check} function for each of the rules, where we input the group of entities under which a relation is defined, the expected observation on that relation, and the actually observed observation in the file we are checking. If the observed instance is inconsistent with the relations built from our training set files, then we would see it reported. In the probablistic setting of our tool, we would implement cutoffs to tune our \textit{check} to ignore observed relations under a certain significance level.

\subsection{Checker}
\label{sec-checker}

With the learned rules generates by the learner module,
  \app checks whether any entry in a target configuration file violates the learned rules and constraints.
\app parses a verification target configuration file the same way employed in the translator for learning,
  obtaining a structured and typed representation.
Then, the checker applies the learner to build the set of relations observed in the file.
For any relation that violates a known rule, the checker will output the predicate and keyword sets associated with that rule, as well as the learned support and confidence values.
Since the tool is probabilistic, we provide the user with these values to determine if they rule must be satisfied in their application on a case-by-case basis.
For instance, the \texttt{key\_buffer} misconfiguration from Sec. \ref{ex:fine} will only be noticeable if the application experiences a heavy traffic load, so the user may choose to ignore this error if they are confident this will not be an issue.


\subsection{Implementation}


