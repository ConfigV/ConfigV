===========================================================================
                            CAV'17 Review #182A
---------------------------------------------------------------------------
Paper #182: Synthesizing Configuration File Specifications with Association
            Rule Learning
---------------------------------------------------------------------------

                      Overall merit: 4. Accept

                         ===== Paper summary =====

This paper addresses the problem of detecting configuration errors in configuration files of complex software systems, e.g., servers and databases. The idea behind the proposed approach is to learn rules from a corpus of sample configuration files. One could then apply those rules to a given configuration and automatically detect when certain fields are, for example, missing.

The presented technique is probabilistic, and assigns confidence to a reported error. The technique has been implemented and applied to real configuration files for MySQL. The training times seem to be reasonable (compared to previous work) and the reported errors are complex.

                      ===== Comments for author =====

Pros:

--> Interesting and very important application domain for "bug" finding

--> Novel technique that seems to be applicable beyond configuration files

--> Convincing experimental evaluation on MySQL

Cons:

--> Experimental evaluation is over only one application's configuration files

Comments:

Overall, I enjoyed reading this paper and I think the idea of learning association rules and imposing probabilities is very interesting for this domain. Configuration errors are extremely annoying and hard to debug -- judging from experience -- particularly in server and cloud software. I'm glad someone is building usable tools to aid in configuration debugging.

While the domain is configuration files, I believe the approach of learning association  rules is potentially adaptable to programs, e.g., by detecting API usage patterns and flagging unlikely ones.

When the rules were first formalized in Section 5, I had a really hard time mapping

r = S â‡’ p(S,T)

to the informal rules presented early on. I strongly suggest that you provide an example instantiation of the formal rule.

At the experimental level, it is unfortunate that false positives are not investigated. It is indeed difficult to go through each reported error and categorize whether it's really a misconfiguration, but it would really have made a stronger story if more investigation is performed. Nonetheless, I think the approach is interesting and I would like to see it at CAV.

===========================================================================
                            CAV'17 Review #182B
---------------------------------------------------------------------------
Paper #182: Synthesizing Configuration File Specifications with Association
            Rule Learning
---------------------------------------------------------------------------

                      Overall merit: 2. Weak reject

                         ===== Paper summary =====

The paper presents VeriConf, a system for checking configuration files
against a set of automatically learned rules.  A configuration file is
a list of key / value pairs, and the rules VeriConf learns are meant
to capture correctness constraints on these pairs.  For example, a
given key should always be associated with integer values, or the
mapping for one key should always precede the mapping for another key.
Given a training set of configuration files, VeriConf first parses
them and assigns a probabilistic type to each key, based on how many
times that key is associated with a value of a basic type (such as
boolean, integer, or size).  Next, VeriConf, applies association
learning to produce a set of correctness rules, which are constructed
from a set of built-in predicates (such as ordering or integer
correlations) and keys found in the training set.  Each learned rule
is also paired with support and confidence values that are then used
to rank the rule's estimated importance / correctness.  After the
learning is done, VeriConf can be used to check previously-unseen
(test) files for conformance to the learned rules.  The evaluation
shows that VeriConf's learning algorithm scales linearly with the size
of the training set, and that it ranks true positives reasonably highly
in 15 configuration files with known errors.

                      ===== Comments for author =====

This paper addresses an important practical problem---learning
configuration-checking rules---and the presented solution is a new
point in the design space of such tools.  In particular, VeriConf can
learn more kinds of rules than ConfigC, which is the closest related
work; it scales better than ConfigC; and it does not require the files
in the training set to be correct (i.e., free of configuration
errors).  The downside, however, is that the rules learned by VeriConf
are not guaranteed to be correct, so they have the potential to lead
to (many) false alarms.  As a result, the evaluation burden for this
kind of tool is high, and the current evaluation doesn't quite meet
it.

Showing practical usefulness for a heuristic technique like VeriConf
usually involves a two-part evaluation:

The first part would demonstrate that VeriConf finds known bugs while
producing few false alarms.  The paper states that false alarms are
difficult to identify in this setting, but for the tool to be useful,
the programmer must be able to at least guess whether a bug report is
real or not.  So, a best-effort attempt at classifying bug reports
would be sufficient here.  The evaluation shows some results of this
kind in Table 2, but important statistics are missing such as the
overall rate of true vs false positives, and the results cover 5 rules
for 3 files each.  What about all the other rules that VeriConf learns
(Table 1 shows 468 learned rules)?

The second part would demonstrate that VeriConf can find
previously-unknown bugs in configuration files from open-source
projects (e.g., in GitHub).  The usual practice is to report these
bugs to the developers and, hopefully, have them accepted as patches.

Overall, I think this is a promising research direction and good work,
but without an extensive evaluation of the sort outlined above, it is
difficult to assess the effectiveness of a tool like VeriConf.

===========================================================================
                            CAV'17 Review #182C
---------------------------------------------------------------------------
Paper #182: Synthesizing Configuration File Specifications with Association
            Rule Learning
---------------------------------------------------------------------------

                      Overall merit: 2. Weak reject

                         ===== Paper summary =====

This paper describes a technique for finding likely software configuration errors. The main idea is to use association rule learning to mine rules from a training corpus. In order to make the association rule learning more successful, the authors perform type inference but associate probabilities with each inferred type. The authors also propose representing the learned rules as a graph and analyze this graph to figure out which rules are likely to be more important. These ideas are implemented in a tool called VeriConf, and the authors report statistics about the number of reports and learned rules.

                      ===== Comments for author =====

This paper addresses a relevant problem (detecting misconfigurations) and the methodology described here is fairly sensible. However, the technical contributions of the paper seem quite thin. The probabilistic type inference seems quite straightforward, and the authors just use standard techniques like association rule learning. So, it is unclear whether the paper contains enough technical contributions to merit a CAV paper. It might be that this paper is a better fit for a software engineering conference.

Another complaint is that the paper is quite poorly-written with many typos and grammatical errors. The authors should proof-read the paper before submitting!

Other comments:

- In the probabilistic type inference rules, you use a threshold to check if the variable should be assigned a certain type. How was this determined in your evaluation?

- I did not understand why all the reports that are removed by introducing probabilistic types are false positives. It would be helpful to elaborate.

- Even though you claim you have a modified version of the association learning algorithm, it is not described in any detail.

