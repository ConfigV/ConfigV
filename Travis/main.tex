
%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.

%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------

\documentclass{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.

% 10pt          To set in 10-point type instead of 9-point.

% 11pt          To set in 11-point type instead of 9-point.

% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
%\usepackage{todonotes}
\usepackage{graphicx}\usepackage{color}
% \usepackage{csquotes}


\begin{document}

\special{papersize=8.5in,11in}\setlength{\pdfpageheight}{\paperheight}\setlength{\pdfpagewidth}{\paperwidth}
\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} \copyrightyear{20yy} \copyrightdata{978-1-nnnn-nnnn-n/yy/mm} \doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the
% traditional copyright transfer agreement.

%\exclusivelicense
% ACM gets exclusive license to publish,
% you retain copyright

%\permissiontopublish           <
% ACM gets nonexclusive license to publish
% (paid open-access papers,
% short abstracts)
\titlebanner{banner above paper title}
% These are ignored unless\preprintfooter{short description of paper}
% 'preprint' option specified.

\title{Rule learning for verification from temporal data}
\subtitle{Plan of Attack}

\authorinfo{Mark Santolucito}           {Yale University}           {mark.santolucito@yale.edu}
%\authorinfo{Name2\and Name3}           {Affiliation2/3}           {Email2/3}

\maketitle

\begin{abstract}
Rule based classification is an effective machine learning technique that yields low misclassification rates.
However, building a rule based system requires manual creation of large databases of logical constraints.
We present a method to generate rule based systems from temporally structured data.
As an demonstration of this algorithm, we plan to implement a learner that automatically generates constraints for the TravisCI testing framework.
The algorithm will utilize Github commit histories to generate logical constraints that allow us to detect potential build errors without actually building, saving valuable programmer and server time.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out\termsterm1, term2
%\keywords keyword1, keyword2

\section{Introduction}

Machine learning is a new direction in verification\cite{Santolucito2016,ETH_Guy}.
However, algorithms such as neural nets and n-gram models lead to probabilistic models of correctness.
While often effective in practice, these do not provide the true guarantees of a traditional verification approach.
In addition, these tools often cannot provide a justification for the output.
Neural nets behave surprisingly similarly to human brains, and providing proofs to conclusions is equally challenging.

Version space learning \cite{lau2000version}.


\section{Availability of Data}

TravisCI has recently released a metadata api to study their tool\cite{API}.

The first use of this API was analyzing the metadata of builds. It was found that \textasciitilde 15-20\% of failed TravisCI builds are due to "errors". My understanding of the API is that this corresponds to a misconfiguration. This is a large enough number that not only will reducing this be more convenient for users, but it could also significantly reduce server costs for Travis. By using the \verb|tr_duration| field from the API, we can also figure out how much server time we can save\cite{API}.


\section{Feasability Anaysis}
\label{sec:feas}

The language of TravisCI configuration files can likely be handled by ConfigC.
ConfigC works on languages with shallow parse trees, and most of the .travis.yml files is shallow.
However, because .travis.yml allows for bash scripts, some error may be out of scope.
We want to identify the frequency of the types of errors - if too many are related to poorly formed bash scripts, a naÃ¯ve application of ConfigC will not suffice.

To confirm that the types of most misconfigurations are in the scope of ConfigC, we should compile a collection of common misconfigurations.
Usually such a task requires a domain expert.
However, in this case we may be able to learn the common errors by using the API and the temporal data from the commits.

We might code an error as the diff between a sequential pair of an erroring commit and a passing commit.
As a new direction, we may also explore how useful this data is for learning/verification directly.
My guess is that is data will be too noisy, but either way, it should give us a sense of the problem.

To build such a database, scrape the .travis.yml files from each commit for a set of repos.
This can be achieved with the \verb|tr_status, git_commit and gh_project_name| fields from the API.

\subsection{Learning from Temporal properties}

An interesting extension to ConfigC on a theoretical side is to look at the temporal relations between commits.
The key observation is that with each commit that changes the build status, we can learn highly localized information about our model.

The first step is to build an intermediate representation of the data we will learn.
For the sake of efficiency, this data must be structured as a shallow tree.
While the travis.yml file can be taken verbatim, we must use a simplification of the program code.
Since we are only interested in the parts of the program effecting build status, we should extract the imported libraries as a list.
We will call this program summary $P_t$, a simplified representation of the program which contains only the information relevant to the learning process.
The subscript is a tag based on the ordered commit history.
To handle branches, add a superscript to indicate the branch, and restart the counter on a branch - merging should be handled easily (not exactly sure how yet).

From this summary we can then build a model $M(P_t)$, which is the full set of possible rules derivable from the model.
These possible rules must be given by the user.
In \cite{Santolucito2016}, these were called Attributes, and express constraints such as, ordering of lines, or substring relations on values.

We will denote the build status of $P_t$ with $B(P_t)$.
In this application, we consider only the passing and erroring build status, denoted $Pass$, and $Err$ respectively.
All status that are not \verb|error|, as defined by the Travis API, will be included as passing.
For brevity, we denote sequences of build statuses with the following notation:

$S(P_t)=Pass \land S(P_{t+1})=Err \implies S(P_{t,t+1}) = PE$

We know that if a build is passing, then there must not exist any errors.
That is, the model must not contain any rules which are breaking.
Note we are not, however, garunteed that any rules from a passing commit are necessary.

$S(P_t)=Pass \implies \forall r \in M(P_t), r \notin Br$

The key insight is that when we commit a break, we can localize the error to one of the lines that changed.
Either we removed something that was necessary, or added something that was breaking.
We use an inclusive disjunction, since a erroring commit can break multiple things at once.
Expressed formally, where ($\setminus$ is the set difference), that is:

\begin{align*}
  S(P_{t,t+1}) = PE \implies \\
  \exists r \in (M(P_{t})\ \setminus M(P_{t+1})), r \in Nec\ \lor \\
  \exists r \in (M(P_{t+1}) \setminus M(P_{t})), r \in Br
\end{align*}


We then can combine all these formulas with conjunctions and send it to an SMT solver and magically get an answer.
While existential set operations can be expensive in general for an SMT solver, in our application this is not the case.
Thanks to the practice of making incremental commits when using source control, these sets will be small and the SMT will be fairly cheap.
In fact, the above implication generalizes to $P_{t,t+n}$, but for efficiency we must require that $M(P_{t})\ \setminus M(P_{t+n})$ is manageably small.
The definition of small here remains to be experimentally determined.

In fact, I think this approach should work generally for any ML problem where we have an oracle (e.g. neural nets, SVMs), but would like justifications for the classification.
Using this as a second stage of a machine learning algorithm might be really interesting.

\section{Implementation}

To build the learning set, we will use the API to build tuples of .travis.yml and repo info.
In addition to the .travis.yml file (collected as above), we will need at a minimum \verb|gh_lang and tr_status|.
With this \verb|(File,gh_lang,tr_status,...)| tuple, we can then (almost) directly apply ConfigC to detect errors in the shallow part of the tree.

The first (and simplest) requirement is a .travis.yml parser, which is an extension of normal yml parsing to handle travis' ability to include bash scripts.
Second, depending on the results from Sec \ref{sec:feas}, we may also require some extra domain knowledge of the types of errors to be encoded as possible rules.
This is a slightly more involved task that requires writing some non-trivial Haskell code - but it really isn't that bad.


% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.
\bibliography{biblio}



\end{document}
