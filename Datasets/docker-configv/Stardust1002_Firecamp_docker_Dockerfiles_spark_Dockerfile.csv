rm ¦spark-${APACHE_SPARK_VERSION}-bin-hadoop${APACHE_HADOOP_VERSION}.tgz :: Filesystem_operation,()
apt-get ¦update :: Dependencies_operation,()
requirements.txt. :: copy_operation,()
PYTHONHASHSEED:0 :: env_operation,()
CONDA_VERSION:4.3.11 :: env_operation,()
conda ¦config¦--prepend¦channels¦https://conda.anaconda.org/conda-forge :: Dependencies_operation,()
APACHE_HADOOP_VERSION:2.7 :: env_operation,()
wget ¦-q¦http://d3kbcqa49mib13.cloudfront.net/spark-${APACHE_SPARK_VERSION}-bin-hadoop${APACHE_HADOOP_VERSION}.tgz :: Dependencies_operation,()
from,java:8.0
additional_setup.txt. :: copy_operation,()
root :: user_operation,()
pip ¦install¦--upgrade¦pip :: Dependencies_operation,()
PATH:/opt/conda/bin:$PATH :: env_operation,()
rm ¦-rf¦/tmp/* :: Filesystem_operation,()
CONDA_HOME:/opt/conda :: env_operation,()
APACHE_SPARK_VERSION:2.1.0 :: env_operation,()
sh ¦./additional_setup.txt :: Build_Execute_operation,()
apt-get ¦install¦-y¦tar¦unzip¦gzip :: Dependencies_operation,()
apt-get ¦install¦-y¦wget :: Dependencies_operation,()
tar ¦xzf¦spark-${APACHE_SPARK_VERSION}-bin-hadoop${APACHE_HADOOP_VERSION}.tgz¦-C¦/usr/local :: Filesystem_operation,()
SHELL:/bin/bash :: env_operation,()
cd ¦/usr/local :: Filesystem_operation,()
/tmp :: workdir_operation,()
pip ¦install¦-r¦./requirements.txt :: Dependencies_operation,()
echo ¦echo¦'export PATH=/opt/conda/bin:$PATH' > /etc/profile.d/conda.sh && wget --quiet https://repo.continuum.io/miniconda/Miniconda3-${CONDA_VERSION}-Linux-x86_64.sh -O ~/miniconda.sh && /bin/bash ~/miniconda.sh -b -p /opt/conda && rm ~/minico... :: Filesystem_operation,()
ln ¦-s¦spark-${APACHE_SPARK_VERSION}-bin-hadoop${APACHE_HADOOP_VERSION}¦spark :: Filesystem_operation,()
