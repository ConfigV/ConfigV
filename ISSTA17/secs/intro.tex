\section{Introduction}

%what is the problem
Configuration files provide easy access to the most critical parameters of software systems in order to quickly tune the behavior system.
This expressive power in a single file creates a large surface for potential errors that can degrade performance or take down systems entirely.
The verification task for configuration files cannot be addressed with traditional software analysis techniques.
There are rarely formal specifications of a correct configuration, 
  the source code of the underlying system may not be available, 
  and the configuration may depend on the external environment such as the network or available hardware.
To address this issue, we propose a method for automated configuration file verification using a data set of labeled examples of correct and incorrect configurations.

%what approach do we take to the problem
In previous work, we used version space learning for configuration file verification.
We extend that work which only works on correct examples, to work on temporally structured correct and incorrect examples by building an SMT formula to find a classification model.
As a demonstration of this algorithm, we plan to implement it to automatically verify TravisCI configurations.
The algorithm will utilize Github histories to allow us to detect potential build errors without actually building, saving valuable programmer and server time.

%why cant we use existing approach
%legibility issue
Machine learning has been used in various software analysis techniques, such as programming-by-example \cite{lau2000version}, invariant synthesis \cite{garg2014ice}, and error detection \cite{Santolucito2016}.
However, many popular machine learning algorithms, such as neural nets and n-gram models, are not designed to provide simple justifications for their classification results.
While effective in practice, the lack of justification for the results limits the applicability to software analysis, where justification is often critical.
For example, in the case of error detection, the system should not only report which files have errors, but also locate the errors.
This justification issue is also called \textit{legibility} of machine learning.

%correctness issue
Additionally, these probabilistic approaches to machine learning cannot be guaranteed to be complete, that is they will always find the error.
We use version space learning to address the need for a formal completeness guarantee in an automated model generation for verification.
Version space learning is a technique for logical constraint classification~\cite{mitchell82}.
As a logical constraint system, version space learning can provide the justification that is difficult to obtain from other methods, but its main weakness is that it cannot handle noisy data.

%What have we already done (labeled correct)
In our previous work~\cite{Santolucito2016} we built a prototype to verify configuration files from a set of correct examples.
The tool, ConfigC, takes as a training set a set of correct configuration files and builds a set of rules describing a language model.
This model is then used to check if a new configuration file adheres to those rules.
If a file is incorrect, the tool identifies the location of error and reports what is incorrect.
For example, ConfigC might output \texttt{Ordering Error: Expected "extension mysql.so"} before \texttt{"extension recode.so"}.
While this prototype's results are promising, the main weakness is that the learning can only use correct configuration files.
Additionally, there is a relatively high false positive rate (marking correct files as incorrect).

%What do we 'plan' to do to extend this (labeled correct and incorrect)
In order to extend ConfigC, in this paper we provide a new description of ConfigC in terms of version space learning.
This description will allow us to understand how the justifications are produced in ConfigC.
We can then extend ConfigC with a new algorithm to decrease the false positive rate, while maintaining the ability to provide justification.

We propose an extension to ConfigC that allows it to handle both correct and incorrect files in the training set.
Additionally, we extend ConfigC with the ability to use structure within the training set.
While the previous training set from ConfigC was a random sampling of configuration files, many other training sets have a structure based on version history.
The algorithm we propose takes advantage of this commonly available, but underutilized structure of training sets for software analysis.
Because most code does not exist in isolation, but changes over time with development, any training set of code from a version control system, like Github, has this rich temporal structure. 
This structure is a partial order over time, and can be used by our proposed algorithm for more effective learning.

%evaluation plans
To test this approach in practice, we plan to implement our algorithm to check for TravisCI\footnote{\url{http://www.travis-ci.com}} configuration errors.
TravisCI is a continuous integration tool connected to Github that allows programmers to automatically run their test suite on every code update (commit).
A user adds a configuration file to the repository that enables TravisCI and specifies build conditions, such as which compiler to use, which dependencies are required, and a set of benchmarks to test.
This ensures the tool can always be automatically built correctly on a fresh machine.

A recent usage study~\cite{API} of TravisCI found that 15-20\% of failed TravisCI builds are due to "errors" - which is the TravisCI name used to mean the configuration file was malformed and the software could not even be built.
Using the data from \cite{API}, we can also learn that since the start of 2014, approximately 88,000 hours of server time was used on TravisCI projects that resulted in an error status.
This number not only represents lost server time, but also lost developer time, as programmers must wait to verify that their work does not break the build.
If these malformed projects could be quickly statically checked on the client side, both TravisCI and its users could benefit.

Our main contributions are as follow:

\begin{enumerate}

\item We give a description of our tool, ConfigC, in the context of version space learning.
\item We propose a new algorithm to handle both incorrect and correct training data, as well as the internal structure of a training set.
\item We propose a real-world application of this approach, and outline the steps needed for an implementation.

\end{enumerate} 
