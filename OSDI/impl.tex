\section{Implementation}

\ennan{We need to describe how do we implement each module. We may need to
have three paragraphs: one is for translator, one is for learner 
and one is for checker.}


\app is primarily implemented in Haskell, but the suspicious values module was written in R. The suspicious value module re implements the main functionality of the Haskell system, and is is a testament to the simplicity of the algorithm and the ease of reproduction. For clarity, we only present the key components of the Haskell implementation here.

The translator is a simple parser for a small grammar, as described in ($\S$\ref{sec-trans}). The implementation presented no interesting challenges. 

The learning and checking steps of the algorithm are designed to allow for extensibility by new users. In order to verify a configuration file against a new type of error, the user must provide a new type that is an instance of the \textit{Attribute} typeclass (\ie define some functions over that type). In particular, for each type of error the use wishes to detect, they must implement three functions; learn, merge, and check. 


\begin{lstlisting}
class Foldable t => RuleSet t a where
  learn :: IRConfigFile -> t a
  merge :: t a -> t a -> t a
  check :: t a -> IRConfigFile -> [Error]
\end{lstlisting}

This typeclass is polymorphic over the data structure as denoted by \lstinline{Foldable t =>}, which means user can choose any data structure they prefer.
In general, the best data structure is a hashmap from key-value pairs to relations, previously defined as a \textit{P\_Rule} in ($\S$\ref{subsec-rules}).

The \lstinline{learn} function will take the intermediate representation of a single configuration file (as generated by the translator), and generate the set of rules that can be derived from that file.
The \lstinline{merge} function will unify two sets of rules, from two separate configuration files, into a single consistent set of rules. For the probabilistic rules, this is simply a union tends to a summation of the  over be a 
The \lstinline{merge} function

The \lstinline{check} function will start will all the rules we have learned, and filter until we only have the rules that have been broken by the file of interest.
We first eliminate the rules that do not pass the probability check defined in ($\S$\ref{subsec-rules}).
Second, we only consider the rules which are relevant to the file - we do not need to check rules about keywords which do not appear in the file.
Lastly, we take the errors as any rule that does not hold over the given file.

A sketch of a typical implementation of these three functions is shown below. Again, the specifics of this implementation will vary based on the type of error being detected. 

\begin{lstlisting}
learn f = 
  count_relation_events (all_line_pairs f)
merge s1 s2 = 
  unionWith sum s1 s2
check rs f = 
  likely_rs  = filter prob_check rs
  related_rs = filter applies_to_file likely_rs
  errors_rs  = filter (is_valid f) related_rs
 in errors
\end{lstlisting}

The core learning algorithm is simply a fold using \lstinline{merge} over the derived rules from running \lstinline{learn} on each file in the learning set.


Since we learn a set of rules on each file in isolation from the other, we have a pleasingly parallel situation.
Haskell allows us to easily take advantage of this by using the parallel mapping library \cite{parallel}, both for translation to the intermediate representation, and for learning the rules on each file.
The merge stage could also be parallelized, using a divide and conquer approach, but this is not a priority since the learning stage only needs to be run once per learning set, and can then be cached (in our case, as a .json file) for fast reading when doing verification.

The tool is freely available for download at \url{--redacted--}.