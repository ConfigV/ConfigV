\section{Introduction}
\label{sec-intro}

Configuration errors are one of the most important root causes of
today's software system failures~\cite{xu15systems, yin11anempirical}.
In their empirical study Yin {\em et al.}~\cite{yin11anempirical} report
that about 31\% system failures were caused by misconfiguration problems and only 20\% were caused by bugs in program code. 
Misconfigurations, in practice, may result in various
system-wide problems, such as security vulnerabilities, 
application crashes, severe disruptions in software
functionality, and incorrect program executions%
~\cite{zhang14encore, yuan11context, xu13do, xu15hey}.  

While many efforts have been proposed 
to check, troubleshoot, diagnose, and repair configuration errors%
~\cite{attariyan10automating,
su07autobash, whitaker04configuration},
those tools mainly try to understand {\emph{what}} caused the 
error -- they are still not on a level of
automatic verification tools used for regular program 
verification~\cite{Leino10Dafny, PiskacWZ14, BobotFMP15} that can
detect errors without executing the code.
Two main obstacles why we cannot simply apply the existing automatic 
tools and techniques to verification of configuration files are: 1) a lack
of a specification which would describe properties of configuration files, and 2) a program structure of configuration files -- they
are mainly a sequence of entries assigning some value to system variables. The language in which configuration files are written does 
not adhere to a specific grammar or syntax. In particular, the
entries in configuration files are untyped. Moreover, there are surprisingly few rules specifying constraints on entries and there
is no explicit structure policy for the entries.
Thus, automated verification of configuration 
files would be highly
desirable~\cite{wang04automatic, zhang14encore, xu15systems}.

To overcome these obstacles, researchers have proposed approaches based on
a statistical analysis and learning%
~\cite{wang04automatic, zhang14encore, yuan11context}
that try to infer the rules and policies 
how configuration files are constructed.
Rather than explicitly specifying entries' types or rules, 
these efforts are focused on learning policies from a sample dataset.
The learned rules are usually constructed along the following way: 
for every entry in a configuration file, 
they check if it deviates from a ``typical'' value, \ie, 
a value computed from a large training set 
consisting of configuration files.
If the entry is significantly different that the typical value, 
they suspect that it could be a potential configuration error.

The downside of these approaches is that their learning approaches 
are limited to simplistic configuration errors, 
such as type errors and syntax errors, or they 
heavily rely on template-based inference~\cite{zhang14encore}. 
%As we mentioned earlier, 
%because configuration files are not written in a typed language 
%with a fixed syntax,
%type errors and syntax errors are relatively rare and easy to detect. 
Many sophisticated configuration errors, 
which are difficult to template in practice, 
cannot be detected by this way. 
As an illustration of such an error, consider the ordering errors, 
which happens when some entries are inserted in a wrong order. 
For example, if in a PHP configuration file 
an entry {\tt extension = mysql.so} appears 
before {\tt extension = recode.so},  
it would lead to a crash error, where the Apache server cannot start 
due to the segmentation fault error. The correct ordering 
should be {\tt extension = recode.so} before 
{\tt extension = mysql.so}~\cite{yin11anempirical}.
These types of errors cannot be detected by existing
learning efforts, 
since it is hard to build a corresponding template~\cite{xu15systems}. 
%we would need to capture ordering relations of all the entries. 
%Some of them can appear in an arbitrary ordering, 
%and the resulting templates would be too large 
%and not efficient for a practical use.

%Offering automatic verification to configuration files -- like
%what we did to programs -- has been advocated as a reasonable means
%to check the correctness of configuration files of interest.
%Nevertheless, it is still an open problem, because 
%1) software configurations are typically written in poorly structured 
%and untyped languages, and 2) writing specifications or constraints
%for configuration verification is non-trivial in practice.


In this paper, we present \app, the first automatic verification framework
for general software configurations. \app is based on a collection of powerful learning 
algorithms, that do not necessarily depend on templates. The learning process takes as input a
 large sample of configuration files. The process is language-agnostic and it works for any type of configuration files, 
but all of the files in the sample need to be of the same type (such as
MySQL or httpd configuration files).
From that sample,
\app learns an abundant set of rules specifying various properties that hold on the given sample. The files in the sample might contain
errors -- we are, therefore, using probabilistic learning to derive a set of accurate rules. 

For practical purposes we use the dataset given at~\cite{configdataset}. Every file in our dataset contains an error, but it is 
always a different error and it only appears in a small percentage of files. Using that insight we were able, with the help of the probabilistic cutoff, to learn an
accurate set of rules. The rules in general specify what properties do variables need to satisfy. One can see this learning process as a
way of deriving  
a specification for configuration files.  Once there is a specification, we can start with formal verification. With these rules we can efficiently check the correctness of the configuration files
of interest and detect potential errors.

The learning process works in two phases. In the first phase, 
\app analyzes the sample dataset and generates a 
well-structured and probabilistically-typed 
intermediate representation. 
In the second phase, \app derives rules and constraints by analyzing
the intermediate representations.  Every learned rule is annotated with a probability depicting in
which percentage of the files the rule was seen as correct. As learned rules we only select those rules whose 
correctness probability is above the set threshold. 

Finally, we checks whether a
given variable




Building such an automatic verification framework for
configuration files, nevertheless, requires addressing several challenges. 
%First, for variables of one type 
%we learn different rules than for variables of different types. 
%For example, for integer variables $X$ and $Y$, 
%we can learn that $X \leq Y$, while 
%for strings we might want to learn a substring relation. 
First, we need to assign a type to every variable in a configuration file during the transformation phase.
However,
the type of a variable cannot always be fully determined 
from a single value. 
For example, an entry {\tt temp\_dir = 300} assigns to variable {\tt temp\_dir} either a directory named ``300'' or sets the size of that directory to 300 (an integer).
Some of existing type inference 
work would report this is an error, because {\tt temp\_dir} should be assigned
an integer~\cite{zhang14encore}. We address this problem by introducing the concept of
{\emph{probabilistic types}}.
Rather than assigning only one variable to a single type, 
we assign several types with their probability distributions. 
The entry in the above example might be assigned 
the following probabilistic type
{\tt \{temp\_dir, 300, [(File, 60\%), (Int, 40\%)]\}}.
Using the probabilistic types model,
we can generate a more accurate language model,
thus significantly improving our verification capabilities.

Second, when learning rules we use very general templates to 
infer them. The user does not need to provide any templates -- they are internally associated to the types.
Nevertheless, in addition to those general templates, we still need specific algorithms to learn
rules that cannot be easily templated. 





\com{First, given the fact that 
different systems' configuration files use various representations, 
the cost of introducing a completely new configuration
language (or representation) to instead of all these existing format looks 
like an impractical goal. This is not only because administrators need 
to learn this new language, but also changes existing system
infrastructure to support such a new language. 
In order to avoid the above issues, we propose a representation,
which is a type-based representation and 
much more structured than existing configuration format, 
but use it as an intermediate layer. We, at the same time,
develop many pluggable parsers
that can transform different systems' configuration representations
into our proposed intermediate representation for the post process.}

From a practical perspective, 
\app introduces no additional burden 
to the users: they can simply use \app to check for errors in their
configuration files. However, they can also easily extend the framework
themselves. The system is designed to be highly modular. If there is a
class of rules that \app is not currently learning, the user can develop
her own templates and learners for that class. The new learner can be
added to \app and this way it can check an additional new set of
errors.

Our \app prototype still has a few limitations:
for example, we cannot handle configuration errors that can be 
triggered during system execution time.
Nevertheless, we believe \app may suggest a practical path
toward automatic and modular language-based configuration verification.
To summarize, this tool paper makes the following contributions:

\com{
Finally, from a systems perspective this is the first approach that {\emph{proactively}} checks 
 the correctness of configuration files. All previous work
~\cite{xu15systems,zhang14encore,yuan11context, wang04automatic,attariyan10automating,
su07autobash,whitaker04configuration} tries to identify the problem after the
failure occurred. Our approach isolates potential errors before the system failure occurs, e.g. before the installation. We can also see \app as a tool that can run in conjunction with existing tools. Pre-analyzed configuration files are already free from language-based errors, and this way the workloads of post-failure forensics at the runtime
is significantly reduced, thus making these tools truly practical.
}


\begin{enumerate}

\item We propose the first automatic configuration verification
framework, \app, that can learn a language model from an example set of 
correct configuration files, and then use this language model to verify 
configuration files of interest.
 
\item \app proposes probabilistic types to assign a confidence 
distribution over a set of types to each entry, 
while generating the intermediate representation. 

\item \app employs a collection of machine learning algorithms to 
enable powerful rule and constraint inference without assistance 
from any pre-defined templates.

\item \app is capable of detecting various tricky errors that cannot
be detected by previous efforts,
including ordering errors, fine-grained value correlation errors, 
entry missing errors, and environment-related errors. 

\item We implement a \app prototype and evaluate it by
conducting comprehensive experiments.

\end{enumerate}
