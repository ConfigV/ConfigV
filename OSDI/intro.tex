\section{Introduction}
\label{sec-intro}

Configuration errors are one of the most important root causes of
today's software system failures~\cite{xu15systems, yin11anempirical}.
In their empirical study Yin {\em et al.}~\cite{yin11anempirical} report
that about 31\% system failures were caused by misconfiguration problems and only 20\% were caused by bugs in program code. 
Misconfigurations, in practice, may result in various
system-wide problems, such as security vulnerabilities, 
application crashes, severe disruptions in software
functionality, and incorrect program executions%
~\cite{zhang14encore, yuan11context, xu13do, xu15hey}.  

While many efforts have been proposed 
to check, troubleshoot, diagnose and repair configuration errors%
~\cite{attariyan10automating,
su07autobash, whitaker04configuration},
those tools mainly try to understand {\emph{what}} caused the error ---
they are still not on a level of
automatic verification tools used for regular program 
verification~\cite{Leino10Dafny, PiskacWZ14, BobotFMP15} that can
detect errors without executing the code.
Two main obstacles why we cannot simply apply the existing automatic 
tools and techniques to verification of configuration files are: 1) a lack
of a specification which would describe properties of configuration files, and 2) a program structure of configuration files -- they
are mainly a sequence of entries assigning some value to system variables. The language in which configuration files are written does 
not adhere to a specific grammar or syntax. In particular, the
entries in configuration files are untyped. Moreover, there are surprisingly few rules specifying constraints on entries and there
is no explicit structure policy for the entries.
Nevertheless, automated verification of configuration 
files, that could detect would be highly
desirable~\cite{wang04automatic, zhang14encore, xu15systems}.

To overcome these obstacles, researchers have proposed approaches based on
a statistical analysis and learning~\cite{wang04automatic, zhang14encore, yuan11context}
that try to infer the rules and policies how configuration files are constructed.
Rather than explicitly specifying entries' types or rules, these research directions are focused
on learning policies from a sample data set.
The learned rules are usually constructed along the following pattern: for every entry in a configuration file, 
we check if it deviates from a ``typical'' value, \ie a value computed from a large training set consisting of configuration files.
If the entry is significantly different that the typical value, we suspect that it could be a potential configuration error.
The downside of this approach is that these learning efforts are limited to simplistic 
configuration errors, such as type errors and syntax errors, or they 
heavily rely on template-based inference~\cite{zhang14encore}. As we pointed out earlier, since configuration files are not written in a typed language with a fixed syntax,
type errors and syntax errors are relatively rare and easy to detect. However, many sophisticated configuration errors, 
which are difficult to template in practice, cannot be detected this way. As an illustration of such an error consider the ordering errors, which
happen when some entries are inserted in a wrong order. 
For example, if in a PHP configuration file an entry {\tt extension = mysql.so} appears 
before {\tt extension = recode.so},  
it would lead to a crash error, where the Apache server cannot start due to the segmentation fault error. The correct ordering 
should be {\tt extension = recode.so} before 
{\tt extension = mysql.so}~\cite{yin11anempirical}.
These types of errors cannot be detected by existing
learning efforts, since it is hard to build a corresponding template ~\cite{xu15systems}: we would need to capture ordering relations of all the entries. Some of 
them can appear in an arbitrary ordering and the resulting templates would be too large and not efficient for a practical use.

%Offering automatic verification to configuration files -- like
%what we did to programs -- has been advocated as a reasonable means
%to check the correctness of configuration files of interest.
%Nevertheless, it is still an open problem, because 
%1) software configurations are typically written in poorly structured 
%and untyped languages, and 2) writing specifications or constraints
%for configuration verification is non-trivial in practice.


In this paper we present \app, the first automatic verification framework
for general software configurations. \app is based a collection of more powerful learning 
algorithms, that do not necessarily depend on templates. The learning process can be seen as a process of deriving  
a specification of configuration files, from a dataset containing a large sample of configuration files. In order to do that
we utilize a more sophisticated language model. Thus, in the first step \app analyzes the given dataset and generates a 
well-structured and probabilisticly-typed 
intermediate representation.




which aims to truly  achieve automatic configuration verification. we need to leverage a collection of more powerful learning 
algorithms, that do not necessarily depend on templates. In order to do that
there should be a more sophisticated language model, to which we would translate configuration files' entries.
Based on  the learned language model we would develop an abundant set of rules covering 
complex configuration errors. The set of learned rules should be accurate and we should be able to efficiently check 
the configuration files
of interest.

Based on the above argument, 
this paper presents \app, the first automatic verification framework
for general software configurations.
In particular, \app's workflow for verifying a given configuration file
can be viewed as a three-step methodology.
Firstly, \app analyzes a dataset containing sample configuration files,
thus generating a well-structured and probabilistically-typed 
intermediate representation.
Secondly, \app derives rules and constraints by analyzing
the intermediate representations, thus building a language model.
Finally, \app uses the resulting language model
to verify the given configuration file and detect potential errors.
Compared with previous efforts,
\app does not necessarily rely on users' templates 
and is capable of detecting tricky configuration errors that
cannot be identified by existing work (details in $\S$\ref{sec-motiv}). 

Building such an automatic verification framework for
configuration files, nevertheless, requires addressing several challenges. 
Firstly, in order to formulate a correct language model, 
we need to infer each entry's type in a given configuration file, but 
the type of a variable cannot always be fully determined 
from a single value. 
For example, an entry {\tt foo = MAX\_SIZE} is most likely
an integer rather than string; however, existing type inference 
work would report this is an error, because {\tt foo} should be assigned
an integer~\cite{zhang14encore}. \jon{Unclear what the error is. Be more precise} We address this problem by introducing 
{\emph{probabilistic types}}.
Rather than assigning only one variable to a single type, 
we assign several types with their probability distributions. 
The entry in the above example might be assigned 
a probabilistic type like 
{\tt \{foo, MAX\_SIZE, [(Int, 60\%), (String, 40\%)]\}}.
With such probabilistic types in hand,
we can generate a more accurate language model,
thus significantly improving our checking capability.

Secondly, without templates, learning rules and constraints is a challenge. 
\ennan{Here, we need to add one-paragraph description 
to illustrate how we learn rules without templates, 
or why we say our learning results are better than existing
learning approach, \eg, EnCore.}

\com{First, given the fact that 
different systems' configuration files use various representations, 
the cost of introducing a completely new configuration
language (or representation) to instead of all these existing format looks 
like an impractical goal. This is not only because administrators need 
to learn this new language, but also changes existing system
infrastructure to support such a new language. 
In order to avoid the above issues, we propose a representation,
which is a type-based representation and 
much more structured than existing configuration format, 
but use it as an intermediate layer. We, at the same time,
develop many pluggable parsers
that can transform different systems' configuration representations
into our proposed intermediate representation for the post process.}

From a practical perspective, 
\app introduces no additional burden 
to the users: they can simply use \app to check for errors in their
configuration files. However, they can also easily extend the framework
themselves. The system is designed to be highly modular. If there is a
class of rules that \app is not currently learning, the user can develop
her own templates and learners for that class. The new learner can be
added to \app and this way it can check an additional new set of
errors.

Our \app prototype still has a few limitations:
for example, we cannot handle configuration errors that can be 
triggered during system execution time.
Nevertheless, we believe \app may suggest a practical path
toward automatic and modular language-based configuration verification.
To summarize, this tool paper makes the following contributions:

\com{
Finally, from a systems perspective this is the first approach that {\emph{proactively}} checks 
 the correctness of configuration files. All previous work
~\cite{xu15systems,zhang14encore,yuan11context, wang04automatic,attariyan10automating,
su07autobash,whitaker04configuration} tries to identify the problem after the
failure occurred. Our approach isolates potential errors before the system failure occurs, e.g. before the installation. We can also see \app as a tool that can run in conjunction with existing tools. Pre-analyzed configuration files are already free from language-based errors, and this way the workloads of post-failure forensics at the runtime
is significantly reduced, thus making these tools truly practical.
}


\begin{enumerate}

\item We propose the first automatic configuration verification
framework, \app, that can learn a language model from an example set of 
correct configuration files, and then use this language model to verify 
configuration files of interest.
 
\item \app proposes probabilistic types to assign a confidence 
distribution over a set of types to each entry, 
while generating the intermediate representation. 

\item \app employs a collection of machine learning algorithms to 
enable powerful rule and constraint inference without assistance 
from any pre-defined templates.

\item \app is capable of detecting various tricky errors that cannot
be detected by previous efforts,
including ordering errors, fine-grained value correlation errors, 
entry missing errors, and environment-related errors. 

\item We implement a \app prototype and evaluate it by
conducting comprehensive experiments.

\end{enumerate}
