
\section{Evaluations}
\label{sec-eval}

We conduct three experiments to evaluate our \app
prototype. Our experiments aim to answer the following
questions.

\begin{itemize}

\item Whether \app can successfully 
  detect real-world configuration errors?

\item Whether \app can correctly detect anomalous values?

\item How long do training and verification last?

\end{itemize}

\input{table}

\subsection{\app's Effectiveness}

We now apply \app to check against real-world misconfiguration problems.
We use a real misconfiguration dataset~\cite{xu15hey}
that are collected from several online forums, \eg, 
Stack Overflow and MySQL forum.
Each configuration file in this dataset was posted by 
programmers or system administrators, 
when they confront the misconfiguration issues in practice.
This dataset contains 261 incorrect MySQL configuration files.

We apply \app to learn and check the configuration files in this
dataset. Then, we observe how many errors \app can detect,
and manually check whether these detected errors are indeed
configuration errors.
For all the 261 configuration files, we found \app is
able to report all the errors correctly.
For 217 of the 261 configuration files, we found \app's 
error detection results are correct and accurate,
\ie, correctly report the real misconfiguration problem.
However, for 44 of them, \app outputs relatively high false positives,
\eg, higher than 50 false positives,
which is not very helpful to users in practice.
The reason is our training set contains too many incorrect configuration
files, which produce a lot of noises to our learning process,
thus resulting in so many false positives.
  
In order to clearly evaluate the effectiveness of \app,
we extract 20 incorrect configuration files from the dataset.
As a case study, we present the problem descriptions of these files, 
the types of their errors, and the outputs produced by \app.
Table~\ref{table-casestudy} details this information.

As shown in Table~\ref{table-casestudy},
we found \app is capable of detecting many configuration errors previous
efforts, \eg, EnCore~\cite{zhang14encore}, cannot deal with.
To the best of our knowledge, no existing effort
is able to detect fine-grained value correlation errors in the 
MySQL configuration files.
For example, the third case in Table~\ref{table-casestudy}
presents \app detects the fine-grained correlation,
{\tt key\_buffer\_size} should be larger than 
{\tt sort\_buffer\_size} * {\tt max\_connections}.


\begin{table*}[t]
\centering
\caption{Sampled benchmarks for anomaly detection}
\label{table-anomaly}
\begin{small}
\begin{tabular}{|l|l|l|l|}
\hline
{\bf ID} & {\bf Problem Description} & {\bf URL} & 
{\bf \app Report}  \\ 
\hline
\hline
1 & MySQL Server has gone away error in Wamp 
& goo.gl/axnezi  
& WARNING: Violated Upper Hampel Rule for 
\\ &  & 
& {\tt max\_allowed\_packet} with value 104857600 
 \\ \hline

2 &  MySQL has abnormally high load for CPU 
& goo.gl/JrRLrR
& WARNING: Violated Upper Hampel Rule for  
\\ & & 
& {\tt sort\_buffer\_size} with value 1048576000 \\
& & & WARNING: Violated Upper Hampel Rule for  
\\ & & 
& {\tt read\_rnd\_buffer\_size} with value 283115520 \\ \hline

3 & User is having performance issues with MySQL 
& goo.gl/34jTB5
& WARNING: Violated Upper Hampel Rule for  
\\ & & 
& {\tt query\_cache\_limit} with value 134217728 \\ \hline

\end{tabular}
\end{small}
\end{table*}

\subsection{Detecting Anomalous Values}

The purpose of checking anomalous values in configuration files 
is to avoid potential performance or workload problems. 
Checking anomalous values is mainly the responsibility of
the second sub-module checker of \app (see $\S$\ref{sec-checker}).

We run \app to check 30 real-world benchmarks from Stack Overflow 
website, and found \app is able to report anomalous values in
these configurations. We also manually check whether these
anomalous values are correct by comparing our results with
the answers on Stack Overflow.

Table~\ref{table-anomaly} shows three examples.
We present the problem description, the link and the output results
of \app. We found these anomalous values are very hard for
administrators or users to detect, because they look correct.
However, as shown in the links posted in Table~\ref{table-anomaly},
such anomalous values typically lead to critical performance
problems in practice.

\subsection{The Run-Time of Verification}

\ennan{Here, we need three pictures. In the first figure, x-axis should
be the number (or the size) of entries in the training dataset, and 
y-axis should be the run-time of parsing. In the second figure,
we need to measure the time of generating rules. }
