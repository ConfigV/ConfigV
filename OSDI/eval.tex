

\input{table}

\begin{table*}[tpb]
\centering
\caption{Sampled configuration files for anomaly detection}
\label{table-anomaly}
\begin{footnotesize}
\begin{tabular}{|l|l|l|l|}
\hline
{\bf ID} & {\bf Problem Description} & {\bf URL} & 
{\bf \app Report}  \\ 
\hline
\hline
1 & MySQL Server has gone away error in Wamp 
& goo.gl/axnezi  
& WARNING: Violated Upper Hampel Rule for 
\\ &  & 
& {\tt max\_allowed\_packet} with value 104857600 
 \\ \hline

2 &  MySQL has abnormally high load for CPU 
& goo.gl/JrRLrR
& WARNING: Violated Upper Hampel Rule for  
\\ & & 
& {\tt sort\_buffer\_size} with value 1048576000 \\
& & & WARNING: Violated Upper Hampel Rule for  
\\ & & 
& {\tt read\_rnd\_buffer\_size} with value 283115520 \\ \hline

3 & User is having performance issues with MySQL 
& goo.gl/34jTB5
& WARNING: Violated Upper Hampel Rule for  
\\ & & 
& {\tt query\_cache\_limit} with value 134217728 \\ \hline

\end{tabular}
\end{footnotesize}
\end{table*}



\section{Evaluations}
\label{sec-eval}

We conduct three experiments to evaluate our \app
prototype. We answer the following
questions.

\begin{itemize}

\item Whether \app can successfully 
  detect real-world configuration errors ($\S$\ref{subsec-effectiveness})?

\item Whether \app can correctly detect anomalous values
($\S$\ref{subsec-anomalous})?

\item How long do training and verification last ($\S$\ref{subsec-time})?

\end{itemize}


\subsection{\app's Effectiveness}
\label{subsec-effectiveness}

We now apply \app to check against real-world misconfiguration problems.
We use a real misconfiguration dataset~\cite{configdataset}
that was collected from several online forums, \eg, 
Stack Overflow and MySQL forums.
Each configuration file in this dataset was posted by 
programmers or system administrators, 
when they confronted misconfiguration issues in practice.
This dataset contains 261 incorrect MySQL configuration files.

We apply \app to learn and check the configuration files in this
dataset. Then, we observe how many errors \app can detect.
We next manually check whether these detected errors are indeed
configuration errors.
For all the 261 configuration files, we found \app is
able to report all the errors correctly.
For 217 of the 261 configuration files, we found \app's 
error detection results are correct and accurate,
\ie, we correctly report the real misconfiguration problems
with few false positives.
For the rest of the 44 cases, 
although \app can detect the errors,
\app outputs relatively high false positives,
\eg, higher than 50 false positives,
which is not very helpful to users in practice.
The reason is our training set contains too many incorrect configuration
files, which produces a lot of noise in our learning process,
thus resulting in so many false positives.
  
In order to clearly evaluate the effectiveness of \app,
we extract 20 incorrect configuration files from the dataset.
\app is able to correctly report these configuration errors,
and these errors cannot be handled by any existing efforts.
Table~\ref{table-casestudy} details this information,
including the problem descriptions of these files, 
error types, and the outputs produced by \app.


As shown in Table~\ref{table-casestudy},
we found \app is capable of detecting many configuration errors previous
efforts, \eg, EnCore~\cite{zhang14encore}, cannot deal with.
To the best of our knowledge, no existing effort
is able to detect fine-grained value correlation errors in 
MySQL configuration files.
For example, the third case in Table~\ref{table-casestudy}
shows that \app can detect the following fine-grained correlation:
{\tt key\_buffer\_size} should be larger than 
{\tt thread\_size} *  {\tt sort\_buffer\_size}.


\subsection{Detecting Anomalous Values}
\label{subsec-anomalous}

The purpose of checking anomalous values in configuration files 
is to avoid potential performance or throughput problems. 
Checking anomalous values is mainly the responsibility of
the second sub-module checker of \app (see $\S$\ref{sec-checker}).

We run \app to check 30 real-world benchmarks from the Stack Overflow 
website, and found \app is able to report anomalous values in
these configurations. We also manually check whether these
anomalous values are correct by comparing our results with
answers on StackOverflow.

Table~\ref{table-anomaly} shows three examples.
We present the problem description, the link, and the output results
of \app. We found these anomalous values are very hard for
administrators or users to detect, because they look correct.
However, as shown in the links posted in Table~\ref{table-anomaly},
such anomalous values typically lead to critical performance
problems in practice.

\subsection{Training \& Verification Performance}
\label{subsec-time}

We run our experiments measuring the run-time of training and
verification of \app.
Our experiments were run on a machine equipped with
64-bit Intel Core i7-4790 CPU @ 3.60GHz and 8 GB of RAM.

\para{Training performance.}
We first evaluate the training time of \app. Namely, we examine how long
does \app need to use to generate rules.
We vary the number of configuration files in our training dataset
between 10 and 200, and record the corresponding time.
Table~\ref{table-training} presents our evaluational results.
We found the training time of \app is acceptable in practice,
because we obtained good enough rules on 150 files in the training dataset.
For 150 files, \app only requires about 7 minutes of runtime.
Furthermore, even if we train \app with a 200-file dataset,
we need less than 15 minutes, but such a training dataset
can generate about 200,000 rules.

\para{Verification performance.}
We also measure the verification time of \app, and found
that the verification time is very fast. With 232,613 rules,
\app needs less than one second to verify a MySQL configuration file.
\app only needs 3 minutes to finish the verification of 
the real-world dataset~\cite{configdataset} we used to 
evaluate the effectiveness of \app,
which contains 261 misconfiguration files. 

\begin{table}[t]
\centering
\caption{Training time for different datasets}
\label{table-training}
\begin{tabular}{|c|c|}
\hline
{\bf The \# of Files for Training}       & {\bf Training Time (sec.)}  \\ 
\hline
\hline
10   & 2.997        \\ \hline
50   & 46.4208          \\ \hline
100  & 206.7798       \\ \hline
150  & 431.7772        \\ \hline
200  & 856.6383  \\ 
\hline
\end{tabular}
\end{table}


