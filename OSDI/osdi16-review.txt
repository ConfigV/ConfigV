===========================================================================
                           OSDI '16 Review #281A
---------------------------------------------------------------------------
Paper #281: An Automatic Verification Framework for Software Configurations
---------------------------------------------------------------------------

How much do you want to see this paper in the conference?:
                                     D. I do not want to see this paper at
                                        OSDI.
Is this paper likely to make readers think differently about the topic?:
                                     C. Might change a few perceptions.
Are others likely to build upon this work in their own projects?:
                                     C. A few groups might build on this
                                        work.
How does this paper compare to others you have reviewed for OSDI 2016?:
                                     E. Bottom 50% of submitted papers

                         ===== Paper summary =====

This paper presents a tool, ConfigV, that can help automatically inferring rules about configuration entries from a set of configuration files and use the rules to detect misconfigurations. ConfigV first parses a configuration file and assigns a probabilistic type to each configuration entry, and it then conducts template-guided rule learning. The evaluation shows that ConfigV can indeed detect many real-world configuration errors.

                        ===== Paper strengths =====

Detecting misconfiguration is an important and challenging task.

ConfigV can detect several complicated types of configuration errors that some previous tools cannot.

                       ===== Paper weaknesses =====

The novelty over previous work is low.

False positives are high for a number of test cases.

                   ===== Comments for the authors =====

ConfigV is targeting an important problem: misconfiguration. The technique of probabilistic type inference is interesting; a few complicated types of misconfigurations targeted by ConfigV, such as fine-grained value correlation and ordering errors, are very interesting and have not been covered by some previous work. ConfigV is evaluated using real-world misconfiguration files and shown to be able to detect many real-world misconfigurations.

My biggest concern is the comparison between ConfigV and previous work.
1. The idea of ConfigV is almost exactly the same as citation 22. 
Citation 22 also uses probabilistic type inference, uses exactly the same system architecture, as shown in Figure 1, and targets almost exactly the same set of misconfiguration problems, including ordering errors and fine-grained value correlation, using the same template-based learning technique.
It looks like the only difference may be: (1) citation 22 requires a rule to be followed by all configuration files during training and (2) citation 22 did not contain as solid a evaluation as this paper.
I feel the delta is too incremental for an OSDI paper.

2. The contribution of ConfigV over previous work like EnCore is incremental.
EnCore also uses type-based template-guided learning, and also tries to infer configuration-entry types automatically. EnCore can also tolerate wrong configuration in its training set (i.e., does not require a rule to be followed by all training configuration files).
The only difference seems to be:
(1) ConfigV assigns probability to configuration-entry types;
(2) ConfigV has used more templates in its learning.

I feel the contribution of (1) probabilistic configuration-entry type is marginal, and is not evaluated.
The new templates used by ConfigV, such as ordering errors and others, are indeed interesting and would be a contribution of ConfigV, if not for citation 22.

Overall, I like ConfigV as a useful misconfiguration checking tool. I believe it will be practically useful. However, the delta over already published papers seems low.

A few other comments:

1. The false positive of ConfigV seems really high for 44 out of 261 configuration files: over 50 false positives. 
What is the reason behind it?
Any change ConfigV can be more accurate?

2. Did you try using different set of configuration files for training and checking?

3. How does ConfigV performance scale with more training files?
Some of these templates such as ordering errors and fine-grained value correlation seem to be expensive to learn.

===========================================================================
                           OSDI '16 Review #281B
---------------------------------------------------------------------------
Paper #281: An Automatic Verification Framework for Software Configurations
---------------------------------------------------------------------------

How much do you want to see this paper in the conference?:
                                     C. I'm not enthusiastic about this
                                        paper being in the program.
Is this paper likely to make readers think differently about the topic?:
                                     D. This paper is not at all
                                        surprising.
Are others likely to build upon this work in their own projects?:
                                     D. It's difficult to see how anyone
                                        could build on this paper.
How does this paper compare to others you have reviewed for OSDI 2016?:
                                     E. Bottom 50% of submitted papers

                         ===== Paper summary =====

The paper describes a verification framework for software configurations, called ConfigV, that relies on a combination of machine learning, type inference, and rules. ConfigV analyzes a large corpus of similar configuration files to infer the structure and semantic properties of a configuration; it subsequently uses this knowledge base to check against test configurations to identify anomalies and deviances from the normal.

                        ===== Paper strengths =====

- The paper is on an important topic in which, in spite of several attempts over the years, the state of the art has not advanced significantly. ConfigV takes one more stab at the problem.

- The idea of probabilistic types is an interesting one.

                       ===== Paper weaknesses =====

- Verification is a strong choice of word for what the paper accomplishes. ConfigV is largely based on heuristics and well-intentioned guesstimates, but calling it verification is pushing it.

- The availability of a large corpus of similar configuration files (e.g., for a particular web service) is not really feasible for a broad range of configurations. Perhaps a large IT company running thousands or millions of machines can have access to such data but I doubt it will make its way to a public domain.

- The paper is light on the technical details which makes it difficult to separate a general contribution from a specific implementation that identified a specific set of configuration errors. 

The paper keeps making claims without giving an insight into its workings. Why does ConfigV work? What is the rationale behind finding out all these errors?

For example, it is unclear how the machine learning step updates its internal state after crunching each new sample configuration file to produce a model. Does it maintain some kind of shared global state? How exactly does it function? 

A similar lack of detail exists throughout the paper.

                   ===== Comments for the authors =====

- I liked the usage of the real world examples to illustrate the challenges in automatic configuration checking.

- The assumption of having access to a large corpus of similar config files is made on the basis of the availability of one dataset. I recommend the authors revisit this assumption or provide more justification of why it is a reasonable one. Description of a few scenarios where this dataset naturally occurs is also going to be useful.

- If a sysadmin uses a centralized configuration manager or a centralized imaging system, isn't it possible that the same config error, if present in the master copy, is also replicated on an entire server farm? Even if the config files are then collected from the server farm, they will all have the same error which the machine learning process will "learn" as the normal. Have you encountered this problem and how do you plan to address it?

- It would be very useful to have an example which shows the step-by-step process of computation by which the learner builds its knowledge base as it traverses the file corpus.

- Until sections 2 and 3, the paper keeps making assertions and claims of what ConfigV can do without providing any details or even an intuition of how it makes it possible. I suggest providing at least a one sentence intuition behind the techniques which you can present later on.

- Developing custom parsers for a wide variety of system configurations can turn out to be a nightmare. It is certainly not a scalable solution.

- The example of 
foo = 300
bar = 300.txt

How do you make the definitive claim that foo is a substring of bar?
What if the "300" is purely coincidental. More generally, how can you look at two independent config values and make a dependency claim?

it is very much possible for foo to be the number 300 (say maximum number of connections) and bar to have a filename of 300.txt (since 300 is the hostname of the machine, or whatever)

- Is the list of types in Table 1 exhaustive? What is the criteria for inclusion?

- The probabilistic cutoff to identify an accurate set of rules seems kind of brittle. How does one set a cutoff?
The constant user-defined p value in sec 5.1 has the same problem. I don't see an elegant way to set it.


- The rules and specifications are learnt through machine learning and then the test config is matched against this specification. Doesn't this introduce a compounding of the error, since now the potentially imprecise rules are being used to identify potentially invalid configuration parameters?


- The evaluation lacks at least one critical bit. How does the quality or performance of the ConfigV system improve with the dataset of files. Is there a minimum requirement to build an effective model? Table 4 only shows training times. BTW, is 200 files a sufficiently large set for training?

===========================================================================
                           OSDI '16 Review #281C
---------------------------------------------------------------------------
Paper #281: An Automatic Verification Framework for Software Configurations
---------------------------------------------------------------------------

How much do you want to see this paper in the conference?:
                                     C. I'm not enthusiastic about this
                                        paper being in the program.
Is this paper likely to make readers think differently about the topic?:
                                     C. Might change a few perceptions.
Are others likely to build upon this work in their own projects?:
                                     C. A few groups might build on this
                                        work.
How does this paper compare to others you have reviewed for OSDI 2016?:
                                     D. Top 50% but not top 25% of
                                        submitted papers

                         ===== Paper summary =====

The title says automatic verification, but the paper is not about that.  Instead, ConfigV is another turn in the genre of "big code" tools to assist in detecting errors by comparing code to a corpus.  This one focuses on simple configuration files (mostly attribute-value property lists) for systems like mysql and...well, it mentions PHP and Apache, but the results are all with mysql.   ConfigV suggests values that may be out of range, and also identifies some more difficult problems like ordering errors.   One interesting twist is that the tool mostly works even with noisy data (errors in the corpus), as long as there are not too many.  The key result is that it succeeds in flagging mysql configuration file errors gleaned from stackoverflow, which other tools did not flag (Table 2).

                        ===== Paper strengths =====

- Useful tool.  Table 2 shows would it does that you could not do before.

                       ===== Paper weaknesses =====

- Some presentation issues make it hard to know how the hard cases really work.
- Seems to rely on relatively small corpus and config files: training considers permutations of keys/values (15 minutes for 200 conf files).
- Substantial focus on the algorithms to train the model, which is tangential to OSDI and may be ad hoc.  We don't know if the algorithms are good, just that they are good enough empirically.
- Some cases throw very high false positives (9.1), reducing the value of the tool.
- Approach is arguably incremental to other work in the genre.

                   ===== Comments for the authors =====

I wanted to like this, but I had some trouble following it due to some notational issues and a slightly scattered presentation.  

But I can see how the "probabilistic types" and value constraints, including relative values, can work.  That seems fairly straightforward but also not so interesting.  The ordering constraints are much harder: where order matters, you have a program and not a configuration file.  I could not glean how the ordering constraints work.

Thanks for the "limitations" section: I think I got more insight from that than any other part of the paper.

The notation in the algorithms does not match the text, e.g., in Section 8.  For example, I don't understand what Q and RM are in algorithm 1.  But I can see that the algorithm is very expensive.

Also, I don't understand the notations describing the various sets, or the discussion of "untyped specifications".   Maybe I missed something.

The discussion of rule templates is also confusing.  They are not "necessarily" required, but they are supported, and the user might add them if needed.  I don't understand what they are or how to know if they are needed, or what the limitations are without them.

In Table 4, how big are the files?

Is the mysql corpus a contribution of the paper?  I know that you only have one corpus, but it should be possible to add to it to evaluate how sensitive the results are to noise in the corpus.  How much noise is there?

The Table captions in the evaluation section are not helpful.

In section 10, in comparing to EnCore it says "ConfigV is a language environment, which can even be used to write configuration files".    That is an unsubstantiated claim.

In 9.2, it is strange to refer to config files with errors as "real-world benchmarks".

    ===== Questions for the authors to address in their rebuttal =====

How sensitive is it to the level of diversity among configurations?
How sensitive is it to the size of the configuration files?  (number of keys, or number of steps for the ordering cases)

===========================================================================
        AuthorRebuttal Response by Ennan Zhai <ennan.zhai@yale.edu>
---------------------------------------------------------------------------
We thank all the reviewers for their time and insightful comments.

** Reviewer A and C: Compared ConfigV with previous work

ConfigV is the first effort (to the best of our knowledge) that can detect ordering errors, fine-grained value correlation error, missing entry error and singular value anomalies. The closest work to ConfigV is EnCore [31] and ConfigC [22], like reviewer A pointed out. While EnCore also uses a template scheme and learning algorithms, EnCore cannot detect any of the above errors, due to the simplicity of the algorithms. Compared with ConfigC, ConfigV can more accurately handle many more misconfiguration cases. Furthermore ConfigC cannot check the fine-grained value correlation error and singular value anomalies, which are not easy to detect in practice. In other words, ConfigC outputs many more false positives in many cases and cannot cover sophisticated errors like fine-grained value correlation error.

** Reviewer C: How sensitive is it to the size of configuration files?

In our evaluation, we found the main performance bottleneck of ConfigV is the training phase rather than verification phase, so that we decided to show the measured training time on different size of configuration files in Table 4. For the verification phase, we found verifying different sizes of files (without considering training time) only produced millisecond-level difference, so that we did not post the results.

** Reviewer C: How sensitive is it to the level of diversity among configurations?

The quality of the learning is in large part determined by the quality (diversity) of the training set. This is a well-documented phenomena in machine learning, though we should have included this in our paper.

** Reviewer B: Missing some details in the design description

In the paper we provide a high level overview of our system, leaving out those details, which are rather technical.  Instead we focused on describing interesting new features, such as probabilistic types, which make our system work well in practice. We apologize that the description of our design was not detailed enough. We will put more efforts in explaining the algorithm better in the paper. Let us try to explain it again: 
- We first parse the given training set of configuration files into the tuples. An important part of this translation is that every keyword has an assigned type (such as Int).
- Every type has associated a list of templates, that do not talk about concrete values, but about a generic values (such as, if you have two integers compare them).
- We then instantiate those generic rules with concrete values from a single input file and we check which rule is correct. In addition we learn other properties which do not have typed templates (such as orderings).
- Once we learned rules for one particular file from the training set, this rule contains properties about concrete keywords.
- We then use learning over the whole training set of input files and learn the probability that some rule is correct.
- We experimented with several complex learning schemes, but it turned out that the computing relative frequency of the rule was sufficient.
After the learning phase, we have a large body of rules (with concrete values), each annotated with the probability of occurring. This phase corresponds to constructing a specification (in a traditional “verification” settings). To check correctness of a new configuration file, we run it against all the learned rules that have sufficiently high probability of occurring.

