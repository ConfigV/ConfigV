
# Running the tool

to build fresh and run

$ cd code
$ cabal install
$ /home/cav/.cabal/bin/ConfigC


to turn on detailed reports

1) open Settings.hs
2) set 'vERBOSE = True'
3) run 'cabal install ; /home/cav/.cabal/bin/ConfigC'

to run with your own configuration file

1) add file to 'user' directory
2) Open Settings.hs
3) set 'bENCHMARKS = False'
4) run 'cabal install ; /home/cav/.cabal/bin/ConfigC'

NB: in user mode, we are always verbose
Because we only have a set of correct mysql config files, we can only verify mysql (see user/error for example)
To verify a different languge you will need a sufficently large and diverse set of config files of that langauge.
You will also need to provide a basic parser by editing Convert.hs

# Guide to Source code

The Learners directory has a module for each type of error that is learned/checked.
This is the main workhorse, the rest of the code is scaffolding for these modules.
Each module here will implement three core methods for an Attribute (a class of possible errors).


class Foldable t => Attribute t a where
  learn :: IRConfigFile -> t a
    Given one configFile, generate a set of rules

  merge :: t a -> t a -> t a 
    Given two sets of rules, create a single consistent set

  check :: t a -> IRConfigFile -> Maybe (t a)
    Given a set of rules and a file, check for errors

# definitions for evaluation

A benchmark passes if the specification error is in the error report, up to a useful equality measure.
The equality measure requires two error to have fail points in common - thereby leading the user to the part of the config file that must be repaired.
A false positive is any error in the report that does not match the specification.
See Main.hs, reportBenchmarkPerformance function for more 
    truePos = head spec `elem` foundErrs
    falsePos = filter ((/=) $ head spec) foundErrs

