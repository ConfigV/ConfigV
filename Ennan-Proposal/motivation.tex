\section{Motivation}
\label{sec:intro}

Configuration errors are one of the most important root causes of
today's software system failures~\cite{xu15systems, yin11anempirical}.
We read in the news that recently there was a problem in accessing  Facebook and Instagram \cite{mashableNews} and a Facebook spokeswoman that this was due to a change to the site's configuration systems.
While program verification is mostly focused on detecting errors 
in code, in their empirical study Yin {\em et al.}~\cite{yin11anempirical} report
that about 31\% of system failures were caused by configuration errors problems and only 20\% were caused by bugs in program code. Configuration 
errors are commonly known in literature under the name misconfigurations. 

Misconfigurations, in practice, may result in various
system-wide problems, such as security vulnerabilities, 
application crashes, severe disruptions in software
functionality, and incorrect program executions%
~\cite{zhang14encore, yuan11context, xu13do, xu15hey}. 

The systems research community has recognized this as an important
problem. In fact, at this year's OSDI conference (Operating Systems
Design and Implementation, a top-tier system conference) a paper on
detecting configuration errors~\cite{xu16early} received the best paper 
award. While many efforts have been proposed to check, troubleshoot, diagnose, and repair configuration errors~\cite{attariyan10automating,
su07autobash, whitaker04configuration},
those tools mainly try to understand {\emph{what}} caused the 
error -- they are still not on a level of
automatic verification tools used for regular program 
verification~\cite{Leino10Dafny, PiskacWZ14, BobotFMP15} that can
detect errors without executing the code.

\subsection{Examples}
\label{sec:intro-examples}

We start by presenting two non-trivial configuration errors
extracted from real-world examples. Although the errors are relatively simple, we call them 
non-trivial, because the majority of existing tools, \eg, learning-based checking
tools~\cite{zhang14encore, wang04automatic}, cannot detect
these configuration errors. Most of the presented examples were found on StackOverflow,
a popular question and answer website for programmers.

\para{Example~1: Ordering errors.} 
When a user configures PHP  to run with the Apache HTTP Server, most 
likely the user will take some already existing configuration files and 
adapt them to suit her needs. The configuration file might contain, among 
others,  the following lines:
\begin{lstlisting}[language=C, xleftmargin=.01\textwidth]
extension = mysql.so
...
extension = recode.so
\end{lstlisting} 
In that case the configuration file will cause the Apache server to 
fail to start due to a segmentation fault error. 
This is because, when using PHP in Apache, the extension {\tt mysql.so} 
depends on {\tt recode.so}, and their relative ordering
is crucial. This is an example of so-called {\em ordering error}.
Yin {\em et al.} report that ordering errors widely exist in
many system configurations, \eg, PHP and MySQL,
and typically lead to multiple system crash events.
However, no existing tool in the systems research can effectively solve 
or detect this problem~\cite{zhang14encore, xu15systems, xu13do}.


\para{Example~2: Fine-grained value correlation error.} 
Next example also comes from a discussion on 
StackOverflow~\cite{correlation}.
The user has configured her MySQL as in the following:
\begin{lstlisting}[language=C, xleftmargin=.01\textwidth]
key_buffer_size = 384M
max_heap_table_size = 128M
max_connections = 64
thread_cache_size = 8
...
sort_buffer_size = 32M
join_buffer_size = 32M
read_buffer_size = 32M
read_rnd_buffer_size = 8M
...
\end{lstlisting} 
The user then complained that her MySQL load was very high, causing the website's
response speed to be very slow.
In this case, {\tt key\_buffer\_size} is used by all the threads
cooperatively, while {\tt join\_buffer} and {\tt sort\_buffer} are 
created by each thread for private use; thus, the maximum amount
of used key buffer, \ie, {\tt key\_buffer\_size}, should be larger than 
{\tt join|sort\_buffer\_size} * {\tt max\_connections}. 
Clearly, in the above example, it does not hold, 
so this misconfiguration causes MySQL to load very slowly.
This type of error is more sophisticated than the simple value correlation that some tools can detect~\cite{yin11anempirical, zhang14encore}.


\subsection{Challenges}
\label{sec:intro-chal}

We believe there are two main obstacles 
why we cannot simply apply the existing automatic 
tools and techniques to verification of configuration files are:
\begin{itemize}
\item a lack
of a specification which would describe properties of configuration files
\item a program structure of configuration files -- they
are mainly a sequence of entries assigning some value to system variables. 
\end{itemize}
The language in which configuration files are written does 
not adhere to a specific grammar or syntax. In particular, the
entries in configuration files are untyped. Moreover, there are surprisingly few rules specifying constraints on entries and there
is no explicit structure policy for the entries.
We believe that those are the reasons why there is no automated verification of configuration 
files although the system community expressed that that would be highly
desirable~\cite{wang04automatic, zhang14encore, xu15systems}.


\subsection{Automated Verification of Configuration Files}
\label{sec:intro-goal}

The goal of this proposal is to develop a fully {\bf {automated 
verification  framework for general software configurations}}. We plan to
overcome the above obstacles by first automatically inferring a
specification for configuration files. It is unrealistic to expect the 
users to write an entire specifications for configuration files on their own. 
This process can easily lead to incomplete or even contradictory 
specifications. Instead, we will learn specification from 
a large sample of 
configuration files. A learning process will take as input this 
sample. The process will be language-agnostic and should work for any kind 
of configuration files,  but all of the files in the sample need to be of 
the same kind (such as MySQL or 
HTTPD configuration files). From that sample, we will learn an abundant set of rules 
specifying various properties that hold on the given sample. The rules, in general, 
specify which properties variables in configuration files need to satisfy. One can see this learning process as 
a way of deriving  a specification for configuration files. With these  rules we can efficiently check 
the correctness of the configuration files of interest and detect 
potential errors. Errors are reported if the configuration file does not 
adhere to a specification.

For practical purposes we plan to use a real-world dataset~\cite{configdataset}. The 
files in the sample might contain
errors, but they are typically different errors and only appear in a small percentage of 
files. We will, therefore, use probabilistic learning to derive a set of accurate rules. 

Building such an automatic verification framework for
configuration files requires addressing several challenges. 
First, in the process of inferring a specification, we have to 
analyze mainly with an untyped, unstructured sequence of assignments.
Thus, we need to develop a suitable language model. We do that by 
``guessing'' a type of variables and then deriving formulas that
describe relationships between these variables. However, the
type of a variable cannot always be fully determined 
from a single value. 
An entry {\tt general\_log = 1} assigns an integer to the 
variable {\tt general\_log}. However, 1 here is a Boolean variable,
which denotes that there will be a general log file.
Another example, which shows how this can easily lead to an error, is 
that sometimes a configuration files 
contain the following entry {\tt general\_log = /var/log/mysql/mysql.log}. This will cause    
\ruzica{what will actually happen in this case}
Some existing type inference 
work would report this is an error, because {\tt temp\_dir} should be assigned
an integer~\cite{zhang14encore}. We address this problem by introducing the concept of
{\emph{probabilistic types}}.
Rather than assigning only one variable to a single type, 
we assign several types over a probability distribution. 
Using probabilistic types,
we aim to generate a more accurate language model,
thus significantly improving our verification capabilities.

Second, we will learn patterns describing variables and their relations. We will associate with each type a set of very general templates. The user does not need to provide any templates -- they are internally associated to the types.
Nevertheless, in addition to those general templates, we still need specific algorithms to learn
rules that cannot be easily templated. 

From a practical perspective, we do not want to introduces any additional burden 
to the user: they can simply use \app to check for errors in their
configuration files. However, they can also easily extend the framework
themselves. The system is designed to be highly modular. If there is a
class of rules that \app is not currently learning, the user can develop
her own templates and learners for that class. The new learner can be
added to \app and this way it can check an additional new set of
errors.

Our \app prototype still has many limitations:
for example, we cannot handle configuration errors that can be 
triggered during system execution time.
Nevertheless, we believe \app may suggest a practical path
toward automatic and modular language-based configuration verification.
To summarize, this tool paper makes the following contributions:


\begin{enumerate}

\item We propose the first automatic configuration verification
framework, \app, that can learn a language model from a sample
dataset, and then use this language model to verify 
configuration files of interest.
 
\item \app proposes probabilistic types to assign a confidence 
distribution over a set of types to each entry, 
while generating the intermediate representation. 

\item \app employs a collection of machine learning algorithms to 
enable powerful rule and constraint inference.

\item \app is capable of detecting various tricky errors that cannot
be detected by previous efforts,
including entry ordering errors, fine-grained value correlation errors, 
missing entry errors, and environment-related errors. 

\item We implement a \app prototype and evaluate it by
conducting comprehensive experiments on real-world dataset.

\end{enumerate}
