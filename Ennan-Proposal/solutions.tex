\section{The Learner Module}
\label{sec-learn}


The goal of the learner module is to derive rules and constraints from
the intermediate representation generated by the translator.
In general, the learner module has two components.
The first component (Sec.~\ref{subsec-rules}) 
learns rules for checking configuration errors like
missing entry errors, ordering errors, 
and fine-grained value correlation errors. 
These errors tend to cause total system failures.
%Once the configuration file has been validated against such rules, 
%the user may choose to invoke a more sensitive constraint checker. 
The second component (Sec.~\ref{subsec-constraints}) aims to derive 
constraints on entries to check for suspicious (or anomalous) values 
that may violate standard practice. These anomalies can cause partial 
degradation of the system, 
such as significant reduction in performance, or even 
total failure~\cite{zhang14encore}.

\iffalse
\subsection{Derivation of ConfigC Rules}

The first component is responsible for learning two categories of rules: 
1) rules that are inferred using templates associated with types 
and 2) rules which we call \emph{untyped specifications},
such as ordering or missing entry constraint.
%Both are rules that must hold over multiple parts of a configuration file. 

%However, as mentioned in Sec.~\ref{sec:prelim}, 
%it is difficult to obtain a set of files 
%that is both guaranteed to be without errors
%and large enough to learn sufficiently many rules.
%This usually requires manual verification of the learning set, 
%which is prone to error.
This only considers a rule if it holds over exactly every file in 
the learning set. This behavior can be formally described in Fig~\ref{fig:configC}.

\begin{figure}[!h]
\begin{small}
\belowdisplayskip=-15pt
\abovedisplayskip=-2pt
\begin{align*}
C ::&\ \text{Correct Learning Set}\\
C =&\ \text{\{Configuration Files in Intermediate Representation\}}\\
LR ::&\ \text{Learned Rules} :: \{\textrm{Rule}\}\\
LR =&\ \{ r\ \mid \forall file \in C,\ holds(r,file) \land \\
&\qquad \ \exists file \in C,\ nontriviallyHolds(r,file)\} 
\end{align*}
\end{small}
\caption{The definition of which rules should be learned by ConfigC}
\label{fig:configC}
\end{figure}

where, the $:: \{Rule\}$ indicates that $LR$ (\ie, the learned rule set) 
is a sets of rules.
The predicate $holds$ indicates that the rule should always be true -
  for instance {\tt max\_connections} $>$ {\tt max\_persistent}.
The predicate $nontriviallyHolds$ indicates that the rule should be true in a meaningful way.
For instance, a file may only contain the keyword {\tt max\_connections} - the previous rule then holds vacuously,
 because we say {\tt max\_connections} is always greater than {\tt max\_persistent} if no value is provided for {\tt max\_persistent} in that file.

\com{
\begin{figure}[!h]
%\begin{subfigure}{.3\textwidth}
%\begin{lstlisting}[mathescape=true]
%n = {}
%for (c in $C$):
%  n1 = M(c)
%  n = merge(n, n1)
%\end{lstlisting}
%\end{subfigure}%
\begin{subfigure}{.07\textwidth}
\underline{File 1}
\begin{lstlisting}[mathescape=true]
A
B
C
\end{lstlisting}
\end{subfigure}%
\begin{subfigure}{.07 \textwidth}
\phantom{R}
\begin{lstlisting}[mathescape=true]
AB
AC
BC
\end{lstlisting}
\end{subfigure}%
\begin{subfigure}{.1\textwidth}
\underline{File 2}
\begin{lstlisting}[mathescape=true]
A
C
B
\end{lstlisting}
\end{subfigure}%
\end{figure}
}

As an example of this process, consider the situation 
where the training set contains two files F1 and F2, 
and the only rule we consider is ordering.
F1 contains keywords in the order $[A,B,C]$, 
and F2 contains the ordered keywords $[A,C,B]$.
F1 will produce the set of possible rules $AB, BC, AC$, 
and F2 will produce $AC, CB, BC$.
The only rule that holds for all the files in the training set is 
then $AC$, so only that rule will be included.


\fi

The approach in our earlier work~\cite{santolucitoCAV} is guaranteed to produced only correct rules.
As opposed to a traditional machine learning approach, we have here a formal guarantee of rule coverage.
Furthermore, by design this approach gives a justification (error location) for the classification of a user file as correct or misconfiguration.
This approach is most closely related to version space learning~\cite{mitchell77}, which we discuss further in Sec.~\ref{sec:travis}.



\subsection{Derivation of ConfigV Rules}
\label{subsec-rules}

In \app's learner module we need to develop a learning mechanism
that is tolerant 
enough to accept a dataset of possibly incorrect configuration files.
Rather than manually correcting each file, 
we extend the ConfigC formalism to run probabilistic learning
on the intermediate representations.
Our probabilistic approaches for learning missing entry, 
ordering, and fine-grained value correlation rules stem 
from existing work with building 
non-probabilistic versions of these rule-learning algorithms. 
For each of these rules, 
we consider all possible permutations of keys that appear in every 
file which are appropriately typed, and for our learning process, calculate the likelihood that each of 
these permutations constitute a rule. 
Broadly speaking, we accept rule patterns that appear frequently enough.

We now describe a formalism how to compute the set of learned
probabilistic rules, denoted with $\mathcal{PR}$.
With $\mathcal{U}$ we denote a set of configuration files in the 
intermediate representation. If a template can be instantiated with
keywords from file $f$ and the obtained rule $r$ holds, we state that with the predicate 
$\mathit{appears}(r,f)$.
If a template is instantiated and we obtain rule $r$, with  the predicate 
$\mathit{holds}(r,f)$ we state that either $\mathit{appears}(r,f)$ holds, or $r$ could not be instantiated with keywords from $f$. With every 
rule $r$ we define two sets:
\begin{align*}
S_A(r)& =\{ \ f \in \mathcal{U}\ | \ \mathit{appears}(r,f)\  \} \\
S_H(r)& =\{ \ f \in \mathcal{U}\  | \ \mathit{holds}(r,f) \ \}
\end{align*}
We define then set $\mathcal{PR}$ as
$$\mathcal{PR} = \{\ r \ \mid \ |S_A(r)| > T_1 \land |S_H(r)| > T_2 \ \} $$
where $T_1$ is a threshold value for considering rules which are true in 
majority of cases, and $T_2$ is a threshold value for considering rules 
for which we have at least some examples that that rule is will 
appear in the files. Finding the exact values of $T_1$ and $T_2$ is 
and their connection to $|\mathcal{U}|$
is a question still to be solved.
Even taking a simple majority voting approach, we must find the cutoff value that will yield the best results in learning.
We plan to try multiple strategies to find these values.
One approach may be to use machine learning for program autotuning, with a tool like OpenTuner~\cite{ansel:pact:2014}.
This requires creating some cost function for which the autotuning can optimize.
Additionally, it is possible that $T_1$ and $T_2$ differ depending on the quality of the learning set and/or the particular type of rule being learned.
Building a clear formalization and a modular tool to explore these questions is major task.

Note that this approach differs from a unsupervised learning setting~\cite{hastie2009unsupervised} where the training set is completely unlabeled and seeks to discover some hidden underlying structure.
Although our set is also unlabeled, we make the assumption that some majority of the training set must be correct for their biggest part.
We are again learning a model which will provide clear justification (error locations) for the classification.
However, with this approach we will not have the formal guarantees of the ConfigC approach.
From previous work, we believe this is a worthwhile trade-off in order to decrease the fasle positive rate.

\iffalse
Note that the formalism relies on Algorithm~\ref{alg:plearn}. The algorithm considers both typed templates and
untyped specifications. As a similar iteration logic is used for both, an $opt$ flag distinguishes between the two situations.
constructRule in Algorithm~\ref{alg:crules} refers to the creation of a rule from a tuple of entries, 
$m = (e_1, e_2, \ldots, e_i, \ldots, e_{n-1})$.
The rule creation is formally represented as $m(k(e_1), k(e_2), \ldots, k(e_i), \ldots, k(e_{n-1}))$. 
In order to determine if the rule satisfied, the relation associated with $m(v(e_1), v(e_2), \ldots, v(e_i), \ldots, v(e_{n-1}))$ is evaluated for truth.
The rule can be template-generated, for instance {\tt max\_connections} > {\tt mysql.max\_persistent}, or
it can be untyped-specification-generated, for instance the ordering that {\tt recode.so} must come before 
{\tt mysql.so}.

The candidate entry set $Q$ in Algorithm~\ref{alg:plearn} 
differs for the two cases.
For typed templates, due to typing restrictions, 
we must first filter and examine entries associated with the same type, in order to check that 
the template is satisfied over appropriately typed argument entries. This manifests itself via $Q = filter(F,\tau)$
in Algorithm~\ref{alg:plearn}. For
untyped specifications, we do not need to adhere to this typing restriction, so we simply set $Q = F$.

%probability set associated with the rules, $\Pi$, contains values
%ranging from 0 to 1, which are then compared against an acceptance threshold for the final output.
%\[
%\{ P\_Rule = (a_j, a_k) | j \neq k \} \rightarrow \{ (R_1, R_2, ... , R_n) \}
%\]

%We can think of each of these $(a_j, a_k)$ as possibly having a different relationship, defined by the set $\{ R_i \}$, which cover the entire outcome space of possible relationships between the two values $(a_j, a_k)$.

%For the entry missing rules, we define $R_1$ as the event that $a_j$ and
%$a_k$ appear together, and $R_2$ to be the event that $a_j$ appears
%without $a_k$, or by the transitive equivalent, $a_k$ appears without
%$a_j$. For the ordering rules, we define $R_1$ as the event that
%$a_j$ appears before $a_k$ and $R_2$ be the event that $a_k$ appears
%before $a_j$. For the value correlation rules, we define $R_1$ as the
%event that $a_j \leq a_k$, $R_2$ the event that $a_j = a_k$, and $R_3$
%the case that $a_j \geq a_k$. Notice that the $R_i$ do not have to be
%disjoint, but only have to union to the entire probability space.

%By examining the learning set, we will derive a distribution of the set $\{R_i\}$ based on how many times we observe an occurrence of each relation. This distribution will then be used at checking time to determine if a user's configuration has broken a likely rule. 

A rule $r$ will be reported as broken if the probability the rule is
correct, $\Pi(r)$, is greater than some user defined constant, $p$. This
constant can be adjust to the user's preference. A small $p$ will
increase the likelihood of finding an error, but also increase the
number of false positives that are reported.
\fi

\subsection{Learning Suspicious Constraints}
\label{subsec-constraints}

In addition to checking for correlation, type- or ordering errors, 
the user may also want to examine if there are anomalous values that 
a keyword is assigned to. Such errors may also cause tricky and critical performance issues that are hard to debug.
Consequently, anomalous values should be flagged and a warning returned
to the user indicating the violation.

We now describe a technique that we plan to 
explore to detect anomalous values for 
numerical attributes. Let $A$ be the set of attributes contained in the 
configuration files in the sample dataset. 
Let $A_n$ be the subset of attributes of $A$ which are numerically typed. 
Then, for each attribute $a \in A_n$, we construct a vector $v_a$ of the 
values corresponding to attribute $a$, seen over the entire sample dataset.
For each $v_a$, we compute 
an interval  $$[\hat{v_a} - 50*MAD(v_a), \hat{v_a} + 50*MAD(v_a)],$$ 
where $\hat{v_a}$ represents the median over the values 
in $v_a$ and $MAD(v_a$) refers to the 
median absolute deviation. 
This is a variant of a standard outlier detection test, namely the Hampel identifier. (Mathematically, $MAD(v_a) = 1.4826* median(|v_a - \hat{v_a}|)$, estimating standard deviation 
for a normal distribution.) 
In the checking phase, as long as the checker finds a value for a numerical 
attribute in the checked file outside of this interval, 
a warning would be printed to the user indicating the violating value, 
the attribute, and the upper or lower Hampel threshold. 

The intuition behind this is that if the user has input a value 
that falls outside of an interval containing values that are considered 
``normal'' over the entire sample dataset, 
that value will probably cause an error, in particular for performance. 
We cannot know for sure if this value will cause an issue. 
For instance, a user might have a machine with 
particularly high-end hardware, 
in which case a value beyond the upper Hampel threshold may be appropriate. 
